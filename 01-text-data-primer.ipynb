{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c817a163-44e1-4caf-8133-7de837dbee96",
   "metadata": {},
   "source": [
    "# **Primer on Working With Text Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38fcd8-84e8-4e55-bdc8-c264fc05e894",
   "metadata": {},
   "source": [
    "Let's start off with checking versions of key libraries, in case these notebooks need to be revisited in the future and changes caused by updates to the underlying toolset are easier to track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646e7885-3509-4f06-9c03-0e8af02b168b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version 2.7.0\n",
      "tiktoken version 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "print(\"torch version\", version(\"torch\"))\n",
    "print(\"tiktoken version\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a1f83e-3fce-40dc-b780-24ed480833d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69229c0b-cec8-4734-99ff-417197e87d2e",
   "metadata": {},
   "source": [
    "## **1. Tokenizing Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60dbe75-333a-4a86-8a02-10ae5b442b57",
   "metadata": {},
   "source": [
    "In the book, Sebastian demonstrates the tokenization of text using Edith Wharton's \"The Verdict\" as an example. I'll be using his provided example as well as samples from my own collection. Starting off with Frederic Bastiat's \"The Law\", which was published in the 19th century, offers a very different style of writing and punctuation compared to more modern writers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eed23c27-db0f-45b4-8c6a-41a3b5cbf4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading sample text\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path  = \"data/the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d81a058-04fa-47c7-bd6d-faba2d330a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    verdict = f.read()\n",
    "\n",
    "print(\"Total number of characters: \", len(verdict))\n",
    "print(verdict[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f8044e0-b5fc-4d7e-a3c2-67a0784a32df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  95988\n",
      " The law perverted! The law—and, in its wake, all the collective forces of the nation—the law, I sa\n"
     ]
    }
   ],
   "source": [
    "# And now for Bastiat's text.\n",
    "with open(\"data/the-law-bastiat.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    law = f.read()\n",
    "\n",
    "print(\"Total number of characters: \", len(law))\n",
    "print(law[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6801409-6048-4228-b40f-33e36cae3c6a",
   "metadata": {},
   "source": [
    "These texts will be tokenized and embedded in an LLM. Initially, a simple tokenizer will be created in the next few cells and then it can be applied to the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f23abcd3-42a4-4908-a251-2e9886330d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', ' ', 'the', ' ', 'grim', ' ', 'darkness', ' ', 'of', ' ', 'the', ' ', 'far', ' ', 'future,', ' ', 'there', ' ', 'is', ' ', 'only', ' ', 'war!']\n"
     ]
    }
   ],
   "source": [
    "# This regex splits on white spaces.\n",
    "test = \"In the grim darkness of the far future, there is only war!\"\n",
    "result = re.split(r'(\\s)', test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0266063a-f350-41f3-b47b-90cffcd58893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', ' ', 'the', ' ', 'grim', ' ', 'darkness', ' ', 'of', ' ', 'the', ' ', 'far', ' ', 'future', ',', '', ' ', 'there', ' ', 'is', ' ', 'only', ' ', 'war', '!', '']\n"
     ]
    }
   ],
   "source": [
    "# The regex should also be able to split on punctuations.\n",
    "result = re.split(r'([,.!]|\\s)', test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b236ce1-c17a-4bcb-a620-35cd774b4cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'grim', 'darkness', 'of', 'the', 'far', 'future', ',', 'there', 'is', 'only', 'war', '!']\n"
     ]
    }
   ],
   "source": [
    "# Now we can strip whitespaces from each item while filtering out the empty strings\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b5f51a-d0d8-48bc-8bcb-bf593a904054",
   "metadata": {},
   "source": [
    "It should be noted that stripping whitespaces can aid in brevity, while also reducing memory requirements, especially when working with LLMS using consumer hardware. However, this practice is usually avoided when the retention of exact sentence structures is important for e.g. working with programming languages where indentation and spacing are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "781f5307-a643-476f-89da-ebe433afd83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world!', 'Is', 'this', 'a', 'test', '?', 'Surely', '--', 'it', 'must', 'be', '.']\n"
     ]
    }
   ],
   "source": [
    "# Also accounting for other punctuation types\n",
    "test = \"Hello, world! Is this a test? Surely--it must be.\"\n",
    "result = re.split(r'([,./:\"?_()\\']|--|\\s)', test)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a476f-f460-4b42-9504-73127e3f8c07",
   "metadata": {},
   "source": [
    "Time to test this tokenization on the text samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de871a9a-e1e6-4641-94c4-0dadae87d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "verdict_preproc = re.split(r'([,./:\"?!_()\\']|--|\\s)', verdict)\n",
    "law_preproc = re.split(r'([,./:\"?!_()\\']|--|\\s)', law)\n",
    "\n",
    "verdict_preproc = [item.strip() for item in verdict_preproc if item.strip()]\n",
    "law_preproc = [item.strip() for item in law_preproc if item.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13a0a860-067f-49b2-8714-aeeee177154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter']\n"
     ]
    }
   ],
   "source": [
    "print(verdict_preproc[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bf97b88-b090-4bbb-a068-d756ccbdca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'law', 'perverted', '!', 'The', 'law—and', ',', 'in', 'its', 'wake', ',', 'all', 'the', 'collective', 'forces', 'of', 'the', 'nation—the', 'law', ',', 'I', 'say', ',', 'not', 'only', 'diverted', 'from', 'its', 'proper', 'direction', ',', 'but', 'made', 'to', 'pursue', 'one', 'entirely', 'contrary', '!', 'The', 'law', 'become', 'the', 'tool', 'of', 'every', 'kind', 'of', 'avarice', ',', 'instead', 'of', 'being', 'its', 'check', '!', 'The', 'law', 'guilty', 'of', 'that', 'very', 'iniquity', 'which', 'it', 'was', 'its', 'mission', 'to', 'punish', '!', 'Truly', ',', 'this', 'is', 'a', 'serious', 'fact', ',', 'if', 'it', 'exists', ',', 'and', 'one', 'to', 'which', 'I', 'feel', 'bound', 'to', 'call', 'the', 'attention', 'of', 'my', 'fellow', 'citizens', '.']\n"
     ]
    }
   ],
   "source": [
    "print(law_preproc[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ade04951-19e0-4e3d-908e-9dfb00869293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens created:\n",
      "The Verdict: 4669\n",
      "The Law: 18807\n"
     ]
    }
   ],
   "source": [
    "# Total number of tokens for each text.\n",
    "print(f\"Tokens created:\\nThe Verdict: {len(verdict_preproc)}\\nThe Law: {len(law_preproc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10884c73-8506-4780-b6c3-83e2eb80b09f",
   "metadata": {},
   "source": [
    "## **2. Converting Tokens Into Token IDs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0a7a1-cc62-4891-94ed-0d64f8903de9",
   "metadata": {},
   "source": [
    "The next step involves the conversion of text tokens into token IDs which can be processed via embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d528f8f-817f-4097-97eb-0e75cc7f20cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size:\n",
      "The Verdict: 1143\n",
      "The Law: 3152\n"
     ]
    }
   ],
   "source": [
    "all_words_verdict = sorted(set(verdict_preproc))\n",
    "all_words_law = sorted(set(law_preproc))\n",
    "\n",
    "vocab_sz_verdict = len(all_words_verdict)\n",
    "vocab_sz_law = len(all_words_law)\n",
    "\n",
    "print(f\"Vocab Size:\\nThe Verdict: {vocab_sz_verdict}\\nThe Law: {vocab_sz_law}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04045657-fe9c-4321-99aa-1a88c1f6b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_ids(x):\n",
    "    return {token:integer for integer, token in enumerate(x)}\n",
    "\n",
    "vocab_verdict = create_token_ids(all_words_verdict)\n",
    "vocab_law = create_token_ids(all_words_law)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b65aac6b-9cc0-47e5-a849-00159942798e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Carlo;', 25)\n",
      "('Chicago', 26)\n",
      "('Claude', 27)\n",
      "('Come', 28)\n",
      "('Croft', 29)\n",
      "('Destroyed', 30)\n",
      "('Devonshire', 31)\n",
      "('Don', 32)\n",
      "('Dubarry', 33)\n",
      "('Emperors', 34)\n",
      "('Florence', 35)\n",
      "('For', 36)\n",
      "('Gallery', 37)\n",
      "('Gideon', 38)\n",
      "('Gisburn', 39)\n",
      "('Gisburns', 40)\n",
      "('Grafton', 41)\n",
      "('Greek', 42)\n",
      "('Grindle', 43)\n",
      "('Grindles', 44)\n",
      "('HAD', 45)\n",
      "('Had', 46)\n",
      "('Hang', 47)\n",
      "('Has', 48)\n",
      "('He', 49)\n",
      "('Her', 50)\n"
     ]
    }
   ],
   "source": [
    "# Displaying a sample containing token ids\n",
    "for i, item in enumerate(vocab_verdict.items()):\n",
    "    print(item)\n",
    "    if i >= 50: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4d0c7c-6ccf-429f-9f38-ba3d3f94757f",
   "metadata": {},
   "source": [
    "`Experiment Note` strip \"The Law\" of ids linked to reference markers. Also, be mindful that integers are also being used to refer to dates.\n",
    "\n",
    "Moving onto creating a simple tokenizer which encodes text into token IDs and also turns token IDs back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44dcc325-c314-49ac-89f0-1fb15bed5eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_2_int = vocab\n",
    "        self.int_2_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, target):\n",
    "        preprocessed = re.split(r'([,./:\"?!_()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_2_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_2_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa21e099-ffad-4a48-b73b-3b12f01482d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[184, 3142, 2810, 362, 1074, 541, 2912, 2872, 1973, 2063, 2885, 1697, 5, 1746, 1741, 2586, 1012, 1, 360, 515, 362, 917, 1268, 5, 362, 3007, 5, 362, 2881, 5, 362, 977, 2063, 2872, 1796, 3142, 3133, 2667, 2872, 612, 2999, 3082, 2713, 2532, 0]\n"
     ]
    }
   ],
   "source": [
    "# Testing the tokenizer using The Law's vocab.\n",
    "t1 = SimpleTokenizerV1(vocab_law)\n",
    "\n",
    "text = \"\"\"If you suggest a doubt as to the morality of these institutions, it is said directly—\n",
    "          \"You are a dangerous experimenter,a utopian, a theorist, a despiser of the laws; \n",
    "          you would shake the basis upon which society rests!\"\"\"\n",
    "id1 = t1.encode(text)\n",
    "print(id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2264b420-8e75-4aed-8818-e3849e7142be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 2, 861, 999, 610, 538, 754, 5, 1139, 603, 5, 1, 68, 7, 39, 862, 1121, 764, 803, 7]\n"
     ]
    }
   ],
   "source": [
    "# Testing the tokenizer using The Verdict's vocab\n",
    "t2 = SimpleTokenizerV1(vocab_verdict)\n",
    "\n",
    "text = \"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "id2 = t2.encode(text)\n",
    "print(id2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a81417-24ff-4212-8459-ad53c512a3f6",
   "metadata": {},
   "source": [
    "Note that unique tokenizers are being used for the two texts since the texts contain words which don't overlap with one-another.\n",
    "\n",
    "Let's decode the IDs back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59d85524-8d9c-415a-a883-6e876901bf43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you suggest a doubt as to the morality of these institutions, it is said directly—\" You are a dangerous experimenter, a utopian, a theorist, a despiser of the laws; you would shake the basis upon which society rests!'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.decode(id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e112fda-2199-458c-9d89-f74f031db5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.decode(id2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2136b28c-d532-4eca-8de5-7e7fe1cbe324",
   "metadata": {},
   "source": [
    "## **3. Adding Special Context Tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45409b8e-0393-4596-a8b9-607e45fbab66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3369c330-15d6-4d10-94f5-df268b4f5a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c43e4-1977-4e25-b489-fb6c7bfb2963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294d639-e95a-486b-bda8-82acee1ed5f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca20e17c-505e-4214-8b87-228f0cf7b7b6",
   "metadata": {},
   "source": [
    "## **4. BytePair Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f281e5-d023-43e9-887c-def1c5128467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eed4093-69c0-4eba-a2eb-cc01d231766e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6607b3-aa2d-4105-ac2b-b43a13b48f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4a7a84-34c5-4c10-90b1-b1f103cd018a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae509c0b-1314-40ff-8bce-33b881e3cfea",
   "metadata": {},
   "source": [
    "## **5. Data Sampling With a Sliding Window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a3b6f-4cc7-4fd5-8f5d-00d09009d179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55fbb62-2815-4f8e-8a09-3772c9ff70b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e504a3-8d0c-425f-a081-41e3964ee0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a96ff1-f25f-462f-9bb3-a88cf1cafb51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75e7318d-d617-4b61-8f2a-8ecc06b39d1c",
   "metadata": {},
   "source": [
    "## **6. Creating Token Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9d9bb-729b-4fa9-8f29-08a95192dd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d0068-dd3c-4d12-888d-34fcf53df1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89cae2a-7b8a-4597-bf7c-2399028e0013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd5c5a-75da-4bbf-9c46-d9d3c52951ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e73ac53-0359-4520-aab0-7a5906ed2b52",
   "metadata": {},
   "source": [
    "## **7. Encoding Word Positions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b24756e-29bb-43b2-b3ca-5e758e0e9072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
