{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c817a163-44e1-4caf-8133-7de837dbee96",
   "metadata": {},
   "source": [
    "# **Primer on Working With Text Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38fcd8-84e8-4e55-bdc8-c264fc05e894",
   "metadata": {},
   "source": [
    "Let's start off with checking versions of key libraries, in case these notebooks need to be revisited in the future and changes caused by updates to the underlying toolset are easier to track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646e7885-3509-4f06-9c03-0e8af02b168b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version 2.7.0\n",
      "tiktoken version 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "print(\"torch version\", version(\"torch\"))\n",
    "print(\"tiktoken version\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a1f83e-3fce-40dc-b780-24ed480833d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69229c0b-cec8-4734-99ff-417197e87d2e",
   "metadata": {},
   "source": [
    "## **1. Tokenizing Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60dbe75-333a-4a86-8a02-10ae5b442b57",
   "metadata": {},
   "source": [
    "In the book, Sebastian demonstrates the tokenization of text using Edith Wharton's \"The Verdict\" as an example. I'll be using his provided example as well as samples from my own collection. Starting off with Frederic Bastiat's \"The Law\", which was published in the 19th century, offers a very different style of writing and punctuation compared to more modern writers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eed23c27-db0f-45b4-8c6a-41a3b5cbf4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading sample text\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path  = \"data/the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d81a058-04fa-47c7-bd6d-faba2d330a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    verdict = f.read()\n",
    "\n",
    "print(\"Total number of characters: \", len(verdict))\n",
    "print(verdict[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f8044e0-b5fc-4d7e-a3c2-67a0784a32df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  95988\n",
      " The law perverted! The law—and, in its wake, all the collective forces of the nation—the law, I sa\n"
     ]
    }
   ],
   "source": [
    "# And now for Bastiat's text.\n",
    "with open(\"data/the-law-bastiat.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    law = f.read()\n",
    "\n",
    "print(\"Total number of characters: \", len(law))\n",
    "print(law[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6801409-6048-4228-b40f-33e36cae3c6a",
   "metadata": {},
   "source": [
    "These texts will be tokenized and embedded in an LLM. Initially, a simple tokenizer will be created in the next few cells and then it can be applied to the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f23abcd3-42a4-4908-a251-2e9886330d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', ' ', 'the', ' ', 'grim', ' ', 'darkness', ' ', 'of', ' ', 'the', ' ', 'far', ' ', 'future,', ' ', 'there', ' ', 'is', ' ', 'only', ' ', 'war!']\n"
     ]
    }
   ],
   "source": [
    "# This regex splits on white spaces.\n",
    "test = \"In the grim darkness of the far future, there is only war!\"\n",
    "result = re.split(r'(\\s)', test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0266063a-f350-41f3-b47b-90cffcd58893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', ' ', 'the', ' ', 'grim', ' ', 'darkness', ' ', 'of', ' ', 'the', ' ', 'far', ' ', 'future', ',', '', ' ', 'there', ' ', 'is', ' ', 'only', ' ', 'war', '!', '']\n"
     ]
    }
   ],
   "source": [
    "# The regex should also be able to split on punctuations.\n",
    "result = re.split(r'([,.!]|\\s)', test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b236ce1-c17a-4bcb-a620-35cd774b4cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'grim', 'darkness', 'of', 'the', 'far', 'future', ',', 'there', 'is', 'only', 'war', '!']\n"
     ]
    }
   ],
   "source": [
    "# Now we can strip whitespaces from each item while filtering out the empty strings\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b5f51a-d0d8-48bc-8bcb-bf593a904054",
   "metadata": {},
   "source": [
    "It should be noted that stripping whitespaces can aid in brevity, while also reducing memory requirements, especially when working with LLMS using consumer hardware. However, this practice is usually avoided when the retention of exact sentence structures is important for e.g. working with programming languages where indentation and spacing are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "781f5307-a643-476f-89da-ebe433afd83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world!', 'Is', 'this', 'a', 'test', '?', 'Surely', '--', 'it', 'must', 'be', '.']\n"
     ]
    }
   ],
   "source": [
    "# Also accounting for other punctuation types\n",
    "test = \"Hello, world! Is this a test? Surely--it must be.\"\n",
    "result = re.split(r'([,./:\"?_()\\']|--|\\s)', test)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a476f-f460-4b42-9504-73127e3f8c07",
   "metadata": {},
   "source": [
    "Time to test this tokenization on the text samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de871a9a-e1e6-4641-94c4-0dadae87d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "verdict_preproc = re.split(r'([,./:\"?!_()\\']|--|\\s)', verdict)\n",
    "law_preproc = re.split(r'([,./:\"?!_()\\']|--|\\s)', law)\n",
    "\n",
    "verdict_preproc = [item.strip() for item in verdict_preproc if item.strip()]\n",
    "law_preproc = [item.strip() for item in law_preproc if item.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13a0a860-067f-49b2-8714-aeeee177154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"', 'The', 'height', 'of', 'his', 'glory', '\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter']\n"
     ]
    }
   ],
   "source": [
    "print(verdict_preproc[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bf97b88-b090-4bbb-a068-d756ccbdca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'law', 'perverted', '!', 'The', 'law—and', ',', 'in', 'its', 'wake', ',', 'all', 'the', 'collective', 'forces', 'of', 'the', 'nation—the', 'law', ',', 'I', 'say', ',', 'not', 'only', 'diverted', 'from', 'its', 'proper', 'direction', ',', 'but', 'made', 'to', 'pursue', 'one', 'entirely', 'contrary', '!', 'The', 'law', 'become', 'the', 'tool', 'of', 'every', 'kind', 'of', 'avarice', ',', 'instead', 'of', 'being', 'its', 'check', '!', 'The', 'law', 'guilty', 'of', 'that', 'very', 'iniquity', 'which', 'it', 'was', 'its', 'mission', 'to', 'punish', '!', 'Truly', ',', 'this', 'is', 'a', 'serious', 'fact', ',', 'if', 'it', 'exists', ',', 'and', 'one', 'to', 'which', 'I', 'feel', 'bound', 'to', 'call', 'the', 'attention', 'of', 'my', 'fellow', 'citizens', '.']\n"
     ]
    }
   ],
   "source": [
    "print(law_preproc[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ade04951-19e0-4e3d-908e-9dfb00869293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens created:\n",
      "The Verdict: 4669\n",
      "The Law: 18807\n"
     ]
    }
   ],
   "source": [
    "# Total number of tokens for each text.\n",
    "print(f\"Tokens created:\\nThe Verdict: {len(verdict_preproc)}\\nThe Law: {len(law_preproc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10884c73-8506-4780-b6c3-83e2eb80b09f",
   "metadata": {},
   "source": [
    "## **2. Converting Tokens Into Token IDs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a0a7a1-cc62-4891-94ed-0d64f8903de9",
   "metadata": {},
   "source": [
    "The next step involves the conversion of text tokens into token IDs which can be processed via embedding layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d528f8f-817f-4097-97eb-0e75cc7f20cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size:\n",
      "The Verdict: 1143\n",
      "The Law: 3152\n"
     ]
    }
   ],
   "source": [
    "all_words_verdict = sorted(set(verdict_preproc))\n",
    "all_words_law = sorted(set(law_preproc))\n",
    "\n",
    "vocab_sz_verdict = len(all_words_verdict)\n",
    "vocab_sz_law = len(all_words_law)\n",
    "\n",
    "print(f\"Vocab Size:\\nThe Verdict: {vocab_sz_verdict}\\nThe Law: {vocab_sz_law}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04045657-fe9c-4321-99aa-1a88c1f6b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_token_ids(x):\n",
    "    return {token:integer for integer, token in enumerate(x)}\n",
    "\n",
    "vocab_verdict = create_token_ids(all_words_verdict)\n",
    "vocab_law = create_token_ids(all_words_law)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b65aac6b-9cc0-47e5-a849-00159942798e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Carlo;', 25)\n",
      "('Chicago', 26)\n",
      "('Claude', 27)\n",
      "('Come', 28)\n",
      "('Croft', 29)\n",
      "('Destroyed', 30)\n",
      "('Devonshire', 31)\n",
      "('Don', 32)\n",
      "('Dubarry', 33)\n",
      "('Emperors', 34)\n",
      "('Florence', 35)\n",
      "('For', 36)\n",
      "('Gallery', 37)\n",
      "('Gideon', 38)\n",
      "('Gisburn', 39)\n",
      "('Gisburns', 40)\n",
      "('Grafton', 41)\n",
      "('Greek', 42)\n",
      "('Grindle', 43)\n",
      "('Grindles', 44)\n",
      "('HAD', 45)\n",
      "('Had', 46)\n",
      "('Hang', 47)\n",
      "('Has', 48)\n",
      "('He', 49)\n",
      "('Her', 50)\n"
     ]
    }
   ],
   "source": [
    "# Displaying a sample containing token ids\n",
    "for i, item in enumerate(vocab_verdict.items()):\n",
    "    print(item)\n",
    "    if i >= 50: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4d0c7c-6ccf-429f-9f38-ba3d3f94757f",
   "metadata": {},
   "source": [
    "`Experiment Note` strip \"The Law\" of ids linked to reference markers. Also, be mindful that integers are also being used to refer to dates.\n",
    "\n",
    "Moving onto creating a simple tokenizer which encodes text into token IDs and also turns token IDs back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44dcc325-c314-49ac-89f0-1fb15bed5eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_2_int = vocab\n",
    "        self.int_2_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, target):\n",
    "        preprocessed = re.split(r'([,./:\"?!_()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_2_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_2_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa21e099-ffad-4a48-b73b-3b12f01482d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[184, 3142, 2810, 362, 1074, 541, 2912, 2872, 1973, 2063, 2885, 1697, 5, 1746, 1741, 2586, 1012, 1, 360, 515, 362, 917, 1268, 5, 362, 3007, 5, 362, 2881, 5, 362, 977, 2063, 2872, 1796, 3142, 3133, 2667, 2872, 612, 2999, 3082, 2713, 2532, 0]\n"
     ]
    }
   ],
   "source": [
    "# Testing the tokenizer using The Law's vocab.\n",
    "t1 = SimpleTokenizerV1(vocab_law)\n",
    "\n",
    "text = \"\"\"If you suggest a doubt as to the morality of these institutions, it is said directly—\n",
    "          \"You are a dangerous experimenter,a utopian, a theorist, a despiser of the laws; \n",
    "          you would shake the basis upon which society rests!\"\"\"\n",
    "id1 = t1.encode(text)\n",
    "print(id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2264b420-8e75-4aed-8818-e3849e7142be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 2, 861, 999, 610, 538, 754, 5, 1139, 603, 5, 1, 68, 7, 39, 862, 1121, 764, 803, 7]\n"
     ]
    }
   ],
   "source": [
    "# Testing the tokenizer using The Verdict's vocab\n",
    "t2 = SimpleTokenizerV1(vocab_verdict)\n",
    "\n",
    "text = \"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "id2 = t2.encode(text)\n",
    "print(id2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a81417-24ff-4212-8459-ad53c512a3f6",
   "metadata": {},
   "source": [
    "Note that unique tokenizers are being used for the two texts since the texts contain words which don't overlap with one-another. This issue will be handled in the next sections.\n",
    "\n",
    "Let's decode the IDs back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "59d85524-8d9c-415a-a883-6e876901bf43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you suggest a doubt as to the morality of these institutions, it is said directly—\" You are a dangerous experimenter, a utopian, a theorist, a despiser of the laws; you would shake the basis upon which society rests!'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.decode(id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e112fda-2199-458c-9d89-f74f031db5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.decode(id2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2136b28c-d532-4eca-8de5-7e7fe1cbe324",
   "metadata": {},
   "source": [
    "## **3. Adding Special Context Tokens**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287c3594-1905-448d-ba8e-5ce3b5e9c938",
   "metadata": {},
   "source": [
    "In the previous section, both tokenizers returned key errors when the texts were swapped because certain words weren't contained in either's vocabulary. Special tokens can provide LLMS with additional context to handle / circumvent these issues. \n",
    "\n",
    "Examples of some special tokens include:\n",
    " - `[BOS]` Beginning of sequence (here sequence usually refers to a text sample).\n",
    " - `[EOS]` End of sequence.\n",
    " - `[PAD]` Padding, which can be used when LLMs havea  batch size greater than 1, comprising multiple texts of different lengths. This ensures that texts have equal length.\n",
    " - `[UNK]` Unknown, indicates words not contained in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e6138d5-9559-4e4e-aac1-ba7bb4bde9c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Dang'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m t = SimpleTokenizerV1(vocab_law)\n\u001b[32m      4\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mDang it!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, target)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,./:\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m?!_()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m      8\u001b[39m preprocessed = [\n\u001b[32m      9\u001b[39m     item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()\n\u001b[32m     10\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m ids = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_2_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpreprocessed\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,./:\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m?!_()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m      8\u001b[39m preprocessed = [\n\u001b[32m      9\u001b[39m     item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()\n\u001b[32m     10\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_2_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Dang'"
     ]
    }
   ],
   "source": [
    "# Swapping texts and tokenizers to see what effect this may have\n",
    "t = SimpleTokenizerV1(vocab_law)\n",
    "\n",
    "text = \"Dang it!\"\n",
    "\n",
    "t.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317a695f-26fd-43b1-811e-6d82e5c0f0ac",
   "metadata": {},
   "source": [
    "As expected, the odds of Bastiat's texts containing \"Dang\" are pretty much zero. The `\"<|unk|>\"` special token can be used as a workaround in such instances. The vocabulary can also be extended to cater to `\"<|endoftext|>\"` tokens as is the case in GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3369c330-15d6-4d10-94f5-df268b4f5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(law_preproc))) # Extending the law's vocab for now\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "# Shifting to a single vocab object\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e11c43e4-1977-4e25-b489-fb6c7bfb2963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3154"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items()) # Results in two additional items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0294d639-e95a-486b-bda8-82acee1ed5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('—But', 3149)\n",
      "('—if', 3150)\n",
      "('—the', 3151)\n",
      "('<|endoftext|>', 3152)\n",
      "('<|unk|>', 3153)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]): \n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4720e801-b553-4042-aac2-1158e487f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altering the tokenizer so that it can handle the new tokens\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_2_int = vocab\n",
    "        self.int_2_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_2_int else\n",
    "            \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_2_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_2_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e49e38f-2d3f-433a-81d5-1b987c2928ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matters of property are best left to the experts <|endoftext|> Q: What is your pledge? - A: My pledge is eternal service.\n"
     ]
    }
   ],
   "source": [
    "# Test the modified tokenizer\n",
    "t = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Matters of property are best left to the experts\"\n",
    "text2 = \"Q: What is your pledge? - A: My pledge is eternal service.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "94a3bec3-7bd3-485e-9aee-5b8d955905a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3153, 2063, 2352, 515, 642, 1811, 2912, 2872, 3153, 3152, 3153, 65, 344, 1741, 3144, 3153, 66, 3153, 67, 65, 3153, 3153, 1741, 3153, 2661, 6]\n"
     ]
    }
   ],
   "source": [
    "enc = t.encode(text)\n",
    "print(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5ba0ea61-eaea-4602-8b00-d6df3033cb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|> of property are best left to the <|unk|> <|endoftext|> <|unk|>: What is your <|unk|>? <|unk|> A: <|unk|> <|unk|> is <|unk|> service.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.decode(enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a7a381-f52a-4934-9e61-486aefcb5ec7",
   "metadata": {},
   "source": [
    "As expected, modern vocabulary and 19th century writings don't mix well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca20e17c-505e-4214-8b87-228f0cf7b7b6",
   "metadata": {},
   "source": [
    "## **4. BytePair Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3147e90-6fd3-479b-91de-b0d74807d7fa",
   "metadata": {},
   "source": [
    "GPT-2 used BytePair encoding as its tokenizer to handle out-of-vocabulary words. It basically lets the model break down unfamiliar words into smaller subword units or individual characters, thereby enabling it to handling out of vocabulary words.\n",
    "\n",
    "The BPE tokenizer used here is from OpenAI's [tiktoken](https://github.com/openai/tiktoken) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "17173c46-5967-4300-8db6-5730793f07d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib, tiktoken\n",
    "\n",
    "print(f\"tiktoken version: {importlib.metadata.version('tiktoken')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3a30d078-0cb1-4391-a01c-f073012895f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0eed4093-69c0-4eba-a2eb-cc01d231766e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19044, 1010, 286, 3119, 389, 1266, 1364, 284, 262, 6154, 220, 50256, 1195, 25, 1867, 318, 534, 13995, 30, 532, 317, 25, 2011, 13995, 318, 15851, 2139, 13]\n"
     ]
    }
   ],
   "source": [
    "# Reusing the concatenated text from the previous section\n",
    "integers = t.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "af6607b3-aa2d-4105-ac2b-b43a13b48f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matters of property are best left to the experts <|endoftext|> Q: What is your pledge? - A: My pledge is eternal service.\n"
     ]
    }
   ],
   "source": [
    "strings = t.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae509c0b-1314-40ff-8bce-33b881e3cfea",
   "metadata": {},
   "source": [
    "## **5. Data Sampling With a Sliding Window**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b676f23-ad69-4fe7-964e-db1707b1245b",
   "metadata": {},
   "source": [
    "LLMs generate one word at a time, so training data needs to be prepared through sampling using sliding windows. This is not dissimilar to autoregressive forecasting methodologies. Each text chunk will require inputs and targets, which are shifted by one position from left to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b55fbb62-2815-4f8e-8a09-3772c9ff70b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95988 21939\n"
     ]
    }
   ],
   "source": [
    "# Lets re-encode text from the law.\n",
    "enc_txt = t.encode(law)\n",
    "print(len(law), len(enc_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "81e504a3-8d0c-425f-a081-41e3964ee0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_txt[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "50a96ff1-f25f-462f-9bb3-a88cf1cafb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[790] ----> 1611\n",
      "[790, 1611] ----> 286\n",
      "[790, 1611, 286] ----> 1196\n",
      "[790, 1611, 286, 1196] ----> 283\n",
      "[790, 1611, 286, 1196, 283] ----> 501\n",
      "[790, 1611, 286, 1196, 283, 501] ----> 11\n",
      "[790, 1611, 286, 1196, 283, 501, 11] ----> 2427\n",
      "[790, 1611, 286, 1196, 283, 501, 11, 2427] ----> 286\n",
      "[790, 1611, 286, 1196, 283, 501, 11, 2427, 286] ----> 852\n",
      "[790, 1611, 286, 1196, 283, 501, 11, 2427, 286, 852] ----> 663\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating sliding window sampling\n",
    "context_size = 10\n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "36f5f860-3fdc-424e-9886-44f5ae30de8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " every ---->  kind\n",
      " every kind ---->  of\n",
      " every kind of ---->  av\n",
      " every kind of av ----> ar\n",
      " every kind of avar ----> ice\n",
      " every kind of avarice ----> ,\n",
      " every kind of avarice, ---->  instead\n",
      " every kind of avarice, instead ---->  of\n",
      " every kind of avarice, instead of ---->  being\n",
      " every kind of avarice, instead of being ---->  its\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating the same on sample text\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(t.decode(context), \"---->\", t.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dab823-195c-4532-8901-9014156d6564",
   "metadata": {},
   "source": [
    "A simple data loader can be implemented to iterate over the input dataset. This will return the inputs and targets, which are shifted by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e9c9231e-4437-4723-baec-9d65d34d60bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and dataloader which extracts chunks from the input text data\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize input text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equivalent to max_length+1!\"\n",
    "\n",
    "        # Sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i+1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fcfca546-a0cf-4ac3-8c27-4be944a95fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataLoaderV1(txt, batch_size=4, max_length=256, stride=128,\n",
    "                shuffle=True, drop_last=True, num_workers=0):\n",
    "\n",
    "    # Init tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3acd2ef3-0fec-443b-85e4-13d761813808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  383,  1099,   583, 13658]]), tensor([[ 1099,   583, 13658,     0]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoaderV1(law, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "715a420e-d17f-477a-9c20-75d338e8197c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 1099,   583, 13658,     0]]), tensor([[  583, 13658,     0,   383]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0901f472-ae39-49a8-9edb-56881b97ada7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[  383,  1099,   583, 13658],\n",
      "        [    0,   383,  1099,   960],\n",
      "        [  392,    11,   287,   663],\n",
      "        [ 7765,    11,   477,   262],\n",
      "        [10098,  3386,   286,   262],\n",
      "        [ 3277,   960,  1169,  1099],\n",
      "        [   11,   314,   910,    11],\n",
      "        [  407,   691, 35673,   422]])\n",
      "\n",
      "Targets:\n",
      " tensor([[ 1099,   583, 13658,     0],\n",
      "        [  383,  1099,   960,   392],\n",
      "        [   11,   287,   663,  7765],\n",
      "        [   11,   477,   262, 10098],\n",
      "        [ 3386,   286,   262,  3277],\n",
      "        [  960,  1169,  1099,    11],\n",
      "        [  314,   910,    11,   407],\n",
      "        [  691, 35673,   422,   663]])\n"
     ]
    }
   ],
   "source": [
    "# Creating batched outputs\n",
    "dataloader = DataLoaderV1(law, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c235ac7-04df-4145-b373-8849cfe1eced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75e7318d-d617-4b61-8f2a-8ecc06b39d1c",
   "metadata": {},
   "source": [
    "## **6. Creating Token Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9d9bb-729b-4fa9-8f29-08a95192dd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d0068-dd3c-4d12-888d-34fcf53df1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89cae2a-7b8a-4597-bf7c-2399028e0013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd5c5a-75da-4bbf-9c46-d9d3c52951ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e73ac53-0359-4520-aab0-7a5906ed2b52",
   "metadata": {},
   "source": [
    "## **7. Encoding Word Positions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b24756e-29bb-43b2-b3ca-5e758e0e9072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
