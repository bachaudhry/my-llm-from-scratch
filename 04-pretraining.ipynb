{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4f2754",
   "metadata": {},
   "source": [
    "# **Pretraining an LLM on Unlabeled Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "543b63a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.1\n",
      "numpy version: 2.2.5\n",
      "tiktoken version: 0.9.0\n",
      "torch version: 2.7.0\n",
      "tensorflow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        \"tensorflow\" # For OpenAI's pretrained weights since earlier models were trained in TensorFlow\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dbfa8a",
   "metadata": {},
   "source": [
    "This notebook covers the following topics in addition to covering a basic training loop for an introduction to model evaluations during LLM pretraining.\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch05_compressed/mental-model--0.webp\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86298966",
   "metadata": {},
   "source": [
    "## **1. Evaluating Generative Text Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a74d0",
   "metadata": {},
   "source": [
    "Let's begin by setting up the LLM's configuration from previous notebooks. We will be retaining the original context length since we have access to a reasonable consumer grade GPU which should easily handle `context_length: 1024`. This is the same configuration as the original GPT-2 model with 124 million parameters.\n",
    "\n",
    "`text_to_token_ids()` and `token_ids_to_text()` are functions which facilitate the conversion between text and token representations. Additionally, it is important to note that modern LLMs **do not** use a dropout rate as well as bias vectors. The dropout rate below reflects the settings used to train the original GPT-2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d2f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.components import GPTModel\n",
    "\n",
    "# GPT configuration from previos NBs\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 1024, # Retaining original context length\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate \n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval(); # Dropout disabled during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01a17bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " In the grim darkness of the far futureï¿½ lobster bump tobacco Unknown confused WEEKirtual Shares sod\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from utils.components import generate_text_simple\n",
    "\n",
    "# Pipeline functions\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # Add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # Removing batch dim\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_content = \"In the grim darkness of the far future\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_content, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11161b8",
   "metadata": {},
   "source": [
    "The generated text is gibberish since the model hasn't been trained yet. We will also be evaluating the quality of text generation in sections below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b4d05",
   "metadata": {},
   "source": [
    "### **1.1 Calculating The Text Generation Loss i.e. Cross-Entropy and Perplexity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecd34b0",
   "metadata": {},
   "source": [
    "For demonstration purposes, we will create two inputs of three tokens each and will then compute vectors containing the probability scores corresponding to each token in the vocabulary. The index of the highest probability score in each vector represents the most likely next token ID which is then returned to generate the required text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ffbc0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ddb890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "    \n",
    "probs = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probs.shape) # Shape ==> (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d42e10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[41449],\n",
      "         [ 1838],\n",
      "         [39347]],\n",
      "\n",
      "        [[10719],\n",
      "         [ 7417],\n",
      "         [18879]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df1d1a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Yad makes php\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e053b",
   "metadata": {},
   "source": [
    "The output tokens don't match the target tokens we created above - as expected. But we also want to know how far the generated tokens are from the correct predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a35293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([1.0347e-05, 3.0490e-05, 3.4867e-05])\n",
      "Text 1: tensor([2.4436e-05, 1.0729e-05, 6.7333e-06])\n"
     ]
    }
   ],
   "source": [
    "# Token probabilities corresponding to the target indices\n",
    "text_idx = 0\n",
    "target_probs_1 = probs[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probs_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probs_2 = probs[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ae5bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-11.4788, -10.3981, -10.2640, -10.6195, -11.4425, -11.9084])\n"
     ]
    }
   ],
   "source": [
    "# Compute log of all token probabilities\n",
    "log_probs = torch.log(torch.cat((target_probs_1, target_probs_2)))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c144d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-11.0186)\n"
     ]
    }
   ],
   "source": [
    "# Computing the average log probability of each token\n",
    "avg_log_probs = torch.mean(log_probs)\n",
    "print(avg_log_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ff640",
   "metadata": {},
   "source": [
    ">- The goal is to make this average log probability as large as possible by optimizing the model weights\n",
    ">- Due to the log, the largest possible value is 0, and we are currently far away from 0\n",
    ">- In deep learning, instead of maximizing the average log-probability, it's a standard convention to minimize the *negative* average log-probability value.\n",
    "\n",
    "**`CROSS ENTROPY`** This measures the difference between two probability distributions - which in this case are the true distributions of the labels and the predicted distributions from the model i.e. the token probabilities generated by the LLM. PyTorch uses the cross-entropy function for discrete outcomes which _is similar to negative average log probability of the target tokens given the model's generated token probabilities_, thus making the two terms interchangeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9799e9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.0186)\n"
     ]
    }
   ],
   "source": [
    "# Minimize the negative average log-probability\n",
    "neg_avg_log_prob = avg_log_probs * -1\n",
    "print(neg_avg_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff311793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Prior to implementing cross entropy, we should check the shape of logits and targets\n",
    "# Logits shape --> (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce95555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n",
      "Cross-Entropy loss: tensor(11.0186)\n"
     ]
    }
   ],
   "source": [
    "# Flattening above tensors for cross entropy. Flattening is done by combining over the batch dimension\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "# Applying cross entropy loss\n",
    "# Previously we, applied the softmax function, selected probability scores corresponding to target ids and\n",
    "# computed the negative average log probabilities. Cross Entropy handles all of these steps.\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "\n",
    "# Printing shapes and cross entropy loss\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)\n",
    "print(\"Cross-Entropy loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f3a0cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "torch.nn.functional.cross_entropy(\n",
      "    input: torch.Tensor,\n",
      "    target: torch.Tensor,\n",
      "    weight: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    size_average: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    ignore_index: int = -\u001b[32m100\u001b[39m,\n",
      "    reduce: Optional[bool] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    reduction: str = \u001b[33m'mean'\u001b[39m,\n",
      "    label_smoothing: float = \u001b[32m0.0\u001b[39m,\n",
      ") -> torch.Tensor\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Compute the cross entropy loss between input logits and target.\n",
      "\n",
      "See :class:`~torch.nn.CrossEntropyLoss` for details.\n",
      "\n",
      "Args:\n",
      "    input (Tensor) : Predicted unnormalized logits;\n",
      "        see Shape section below for supported shapes.\n",
      "    target (Tensor) : Ground truth class indices or class probabilities;\n",
      "        see Shape section below for supported shapes.\n",
      "    weight (Tensor, optional): a manual rescaling weight given to each\n",
      "        class. If given, has to be a Tensor of size `C`\n",
      "    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
      "        the losses are averaged over each loss element in the batch. Note that for\n",
      "        some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
      "        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
      "        when reduce is ``False``. Default: ``True``\n",
      "    ignore_index (int, optional): Specifies a target value that is ignored\n",
      "        and does not contribute to the input gradient. When :attr:`size_average` is\n",
      "        ``True``, the loss is averaged over non-ignored targets. Note that\n",
      "        :attr:`ignore_index` is only applicable when the target contains class indices.\n",
      "        Default: -100\n",
      "    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
      "        losses are averaged or summed over observations for each minibatch depending\n",
      "        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
      "        batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
      "    reduction (str, optional): Specifies the reduction to apply to the output:\n",
      "        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
      "        ``'mean'``: the sum of the output will be divided by the number of\n",
      "        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
      "        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
      "        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
      "    label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n",
      "        of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n",
      "        become a mixture of the original ground truth and a uniform distribution as described in\n",
      "        `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n",
      "\n",
      "Shape:\n",
      "    - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
      "      in the case of `K`-dimensional loss.\n",
      "    - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\n",
      "      :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.\n",
      "      If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.\n",
      "\n",
      "    where:\n",
      "\n",
      "    .. math::\n",
      "        \\begin{aligned}\n",
      "            C ={} & \\text{number of classes} \\\\\n",
      "            N ={} & \\text{batch size} \\\\\n",
      "        \\end{aligned}\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> # Example of target with class indices\n",
      "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
      "    >>> target = torch.randint(5, (3,), dtype=torch.int64)\n",
      "    >>> loss = F.cross_entropy(input, target)\n",
      "    >>> loss.backward()\n",
      "    >>>\n",
      "    >>> # Example of target with class probabilities\n",
      "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
      "    >>> target = torch.randn(3, 5).softmax(dim=1)\n",
      "    >>> loss = F.cross_entropy(input, target)\n",
      "    >>> loss.backward()\n",
      "\u001b[31mFile:\u001b[39m      ~/code/nbs/LLMs-from-scratch/.venv/lib/python3.11/site-packages/torch/nn/functional.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "torch.nn.functional.cross_entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7557c90c",
   "metadata": {},
   "source": [
    "**`Perplexity`** This is a measure used in conjuction with cross entropy loss to evaluate the performance of models in tasks like language modeling. It can provide a more interpretable way to understand the uncertainty of a model in predicting the next sequence. Specifically, it measures how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset. Lower perplexity indcates that the model predictions are closed to the actual distribution.\n",
    "\n",
    "This measure is often considered to be more interpretable than the raw loss value because it signifies the effective vocabulary size that the model is _unsure_ of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d70dc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: tensor(60995.6328)\n"
     ]
    }
   ],
   "source": [
    "# Calculating the perplexity \n",
    "perplexity = torch.exp(loss)\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6110d4",
   "metadata": {},
   "source": [
    "### **1.2 Calculating Training and Validation Losses** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c97b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Loading sample training text.\n",
    "file_path = \"data/the-law-bastiat.txt\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57b0673a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The law perverted! The lawâand, in its wake, all the collective forces of the nationâthe law, I sa \n",
      "\n",
      "egunâreject all systems, and try libertyâliberty, which is an act of faith in God and in His work. \n"
     ]
    }
   ],
   "source": [
    "# First and last99 chars\n",
    "print(text_data[:99], \"\\n\")\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2ea6491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters:  95988\n",
      "Tokens:  21939\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters: \", total_characters)\n",
    "print(\"Tokens: \", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37d7cc",
   "metadata": {},
   "source": [
    "Here, we will be training our model with training data presented in similarly sized chunks for simplicity and efficiency. In practice, it can also be beneficial to train an LLM with variable-length inputs to help it to better generalize across different types of inputs when it is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f534f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.components import create_dataloader_v1\n",
    "\n",
    "# Training / validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc5602c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running sanity check\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader!\")\n",
    "    \n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81d18ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 1024]) torch.Size([2, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Checking shapes of train and val loaders.\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8526bdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 18432\n",
      "Validation tokens: 2048\n",
      "All tokens: 20480\n"
     ]
    }
   ],
   "source": [
    "# Checking token sizes\n",
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "    \n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the loss of a single batch\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "# Calculate the loss over all batches sampled by a given data loader.\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce num of batches to match the total number of batches in the data loader\n",
    "        # in case values are exceeded.\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else: \n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8668e2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.978011661105686\n",
      "Validation loss: 10.971200942993164\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "    \n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a6e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
