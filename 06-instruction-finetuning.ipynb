{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a279df",
   "metadata": {},
   "source": [
    "# **Finetuning to Follow Instructions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a009fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version:2.2.5\n",
      "matplotlib version:3.10.1\n",
      "tiktoken version:0.9.0\n",
      "torch version:2.7.0\n",
      "tqdm version:4.67.1\n",
      "tensorflow version:2.19.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"numpy\",       \n",
    "    \"matplotlib\", \n",
    "    \"tiktoken\",    \n",
    "    \"torch\",      \n",
    "    \"tqdm\",        \n",
    "    \"tensorflow\",\n",
    "]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version:{version(p)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb2a456",
   "metadata": {},
   "source": [
    "## **1. Instruction Finetuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c99c0",
   "metadata": {},
   "source": [
    "Add Prose..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47314e07",
   "metadata": {},
   "source": [
    "## **2. Dataset Preparation for Supervised Finetuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532cb7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, requests\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    if not os.path.exists(file_path):\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        text_data = response.text\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "            \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c9f279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries 1100\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/instruct/instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
    "    \"/main/ch07/01_main-chapter-code/instruction-data.json\" \n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "586ee0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05bb6410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example entry:\n",
      " {'instruction': \"Provide the past participle form of 'choose.'\", 'input': '', 'output': \"The past participle form of 'choose' is 'chosen.'\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Another example entry:\\n\", data[444])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b222e204",
   "metadata": {},
   "source": [
    "- Items in the downloaded `data` list are stored as dictionaries.\n",
    "- Entries may contain empty `input` fields. \n",
    "- There are a number of ways to format the entries as inputs to the LLM; Two example formats that were used for training the Alpaca (https://crfm.stanford.edu/2023/03/13/alpaca.html) and Phi-3 (https://arxiv.org/abs/2404.14219) LLMs are illustrated below.\n",
    "    - Alpaca-style prompt formatting was the original prompt template for instruction fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7832282",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/04.webp?2\" width=640px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "381c0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceeding with the Alpaca style prompt formatting.\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a reponse that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    \n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    \n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ead8a069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a reponse that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion \n",
      "\n",
      "#### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of a formatted response with input fields\n",
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n#### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input, desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1590b863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a reponse that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Provide the past participle form of 'choose.' \n",
      "\n",
      "#### Response:\n",
      "The past participle form of 'choose' is 'chosen.'\n"
     ]
    }
   ],
   "source": [
    "# Example where the input field is empty\n",
    "model_input = format_input(data[444])\n",
    "desired_response = f\"\\n\\n#### Response:\\n{data[444]['output']}\"\n",
    "\n",
    "print(model_input, desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cce718",
   "metadata": {},
   "source": [
    "- Diving the dataset into `training`, `validation` and `test` sets before passing them to the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3da2ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = int(len(data) * 0.85)\n",
    "test_set = int(len(data) * 0.1)\n",
    "val_set = len(data) - train_set - test_set\n",
    "\n",
    "train_data = data[:train_set]\n",
    "test_data = data[train_set : train_set + test_set]\n",
    "val_data = data[train_set + test_set:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0030138e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8037ebe2",
   "metadata": {},
   "source": [
    "## **3. Organizing Data into Training Batches**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0509d9b",
   "metadata": {},
   "source": [
    "Dataset batching is summarized in the figure below..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e740472",
   "metadata": {},
   "source": [
    "- We tackle this dataset batching in several steps, as summarized in the figure below\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/06.webp?1\" width=640px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d228fea5",
   "metadata": {},
   "source": [
    "- We will implement an `InstructionDataset` class which pretokenizes all inputs in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7408cad1",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/07.webp?1\" width=640px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38baea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        \n",
    "        # Pre-tokenizing the text\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34acf66",
   "metadata": {},
   "source": [
    "- As before, all input batches will be padded to a similar length.\n",
    "- We will be using the `<|endoftext|>` token as a padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52360227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a20d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Developing a custom collate function that can be passed to the dataloader.\n",
    "# Its purpose is to pad the training examples in each batch to have the same length\n",
    "# Note that different batches may vary in length.\n",
    "def collate_draft_1(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    # and increase the max length by +1 to add an extra\n",
    "    # padding token.\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    \n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst = []\n",
    "    \n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add the padding token\n",
    "        new_item += [pad_token_id] \n",
    "        # Pad sequences to batch_max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] * \n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # padded[:-1] allows us to remove the extra padded token\n",
    "        # which was added with +1 in batch_max_length. We will\n",
    "        # circle back to the extra padding token in later sections.\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        inputs_lst.append(inputs)\n",
    "    \n",
    "    # Convert list of inputs to tensor and transfer to target device.\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98275955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5], [6, 7], [8, 9, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_1 = [0, 1, 2, 3, 4, 5]\n",
    "inp_2 = [6, 7]\n",
    "inp_3 = [8, 9, 10]\n",
    "\n",
    "batch = (\n",
    "    inp_1,\n",
    "    inp_2,\n",
    "    inp_3\n",
    ")\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91581126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4,     5],\n",
      "        [    6,     7, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9,    10, 50256, 50256, 50256]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb728185",
   "metadata": {},
   "source": [
    "- The collate function above only handles inputs to an LLM. To allow training for instruction following, we will need the target values as well.\n",
    "- Similar to pretraining an LLM, targets will simply be inputs shifted by 1 position to the right to allow next-token predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b3afa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying the collate function\n",
    "\n",
    "def collate_draft_2(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    \n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    \n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add padding token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1]) # Truncate last token for inputs\n",
    "        targets = torch.tensor(padded[1:]) # Shift +1 to the right for targets\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "        \n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f629c4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4,     5],\n",
      "        [    6,     7, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9,    10, 50256, 50256, 50256]], device='cuda:0')\n",
      "tensor([[    1,     2,     3,     4,     5, 50256],\n",
      "        [    7, 50256, 50256, 50256, 50256, 50256],\n",
      "        [    9,    10, 50256, 50256, 50256, 50256]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0277fed",
   "metadata": {},
   "source": [
    "- Introducing an `ignore_index` value to replace all padding token IDs with a new value. This new value will be ignored during loss calculations.\n",
    "- In this case, we will be replacing `50256` with `-100`.\n",
    "- We will also introduce `allowed_max_length` in case we want to limit the length of the samples. \n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/11.webp?1\" width=640px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0d44a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a29fda60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cuda\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs and targets\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    \n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        \n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "        \n",
    "        # Replace all except the first padding tokens in targets using ignore_index\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index\n",
    "        \n",
    "        # Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "            \n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "    \n",
    "    # Convert list of inputs and targets to tensors and transfer to target device\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    \n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf6c3400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4,     5],\n",
      "        [    6,     7, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9,    10, 50256, 50256, 50256]], device='cuda:0')\n",
      "tensor([[    1,     2,     3,     4,     5, 50256],\n",
      "        [    7, 50256,  -100,  -100,  -100,  -100],\n",
      "        [    9,    10, 50256,  -100,  -100,  -100]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs, targets = collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd648c1",
   "metadata": {},
   "source": [
    "### 3.1 The Role of `-100` in Ignoring Padding Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699abbaf",
   "metadata": {},
   "source": [
    "- Assuming a trivial classification task with 2 class labels, 0 and 1.\n",
    "- Calculating the loss based on the logit values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "689dcafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5]]\n",
    ")\n",
    "targets_1 = torch.tensor([0, 1])\n",
    "\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a291d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7936)\n"
     ]
    }
   ],
   "source": [
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "369a09fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1269)\n",
      "loss_1 == loss_3:  tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# Replacing the class label of one of the examples with -100\n",
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3: \", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e2b17",
   "metadata": {},
   "source": [
    "- This shows that cross-entropy loss function ignored the training example with `-100` label.\n",
    "- PyTorch, by default, has the `cross_entropy(..., ignore_index=-100)` setting to ignore examples corresponding to the -100 label.\n",
    "- This allows us to ignore the end-of-text / padding tokens in the training batches.\n",
    "- Do note that we **don't** want to ignore the first instance of the end-of-text padding token (50256) because it serves as a signal to the LLM that the reponse is complete.\n",
    "- Target token IDs, which correspond to instructions, are commonly masked out as well.\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch07_compressed/13.webp\" width=640px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58b5821",
   "metadata": {},
   "source": [
    "## **4. Creating DataLoaders for the Instruction Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8596d4fb",
   "metadata": {},
   "source": [
    "- `collate_fn` allows us to move the data to the `gpu` directly, thereby bypassing the main the training loop for added efficiency.\n",
    "- The `partial` function will be used to create a new function with a pre-filled `device` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f457e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ef3e746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deef3bde",
   "metadata": {},
   "source": [
    "- `collate_fn` will be used in the batching process once the dataloaders are instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b9fe992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45a6b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15874b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader:\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 92]) torch.Size([8, 92])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 90]) torch.Size([8, 90])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 92]) torch.Size([8, 92])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 92]) torch.Size([8, 92])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 84]) torch.Size([8, 84])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n"
     ]
    }
   ],
   "source": [
    "# Viewing dimensions of the resulting input and target batches\n",
    "print(\"Train Loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db978119",
   "metadata": {},
   "source": [
    "- All batches have a batch size of 8, but varying lengths. We will also double check that the inputs contain `<|endoftext|>` padding tokens corresponding to token ID `50256`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cb74616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
      "          257,  1128,  2591,   326, 20431, 32543,   262,  2581,    13,   198,\n",
      "          198, 21017, 46486,    25,   198, 30003,  6525,   262,  6827,  1262,\n",
      "          257,   985,   576,    13,   198,   198, 21017, 23412,    25,   198,\n",
      "          464,  5156,   318,   845, 13779,    13,   198,   198, 21017, 18261,\n",
      "           25,   198,   464,  5156,   318,   355, 13779,   355,   257,  4936,\n",
      "           13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "       device='cuda:0') \n",
      " 8\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0], \"\\n\", len(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98094bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,   257,\n",
      "         1128,  2591,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n",
      "        21017, 46486,    25,   198, 30003,  6525,   262,  6827,  1262,   257,\n",
      "          985,   576,    13,   198,   198, 21017, 23412,    25,   198,   464,\n",
      "         5156,   318,   845, 13779,    13,   198,   198, 21017, 18261,    25,\n",
      "          198,   464,  5156,   318,   355, 13779,   355,   257,  4936,    13,\n",
      "        50256,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c15104",
   "metadata": {},
   "source": [
    "## **5. Loading a Pre-trained LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb2c7e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-05 13:19:56.412978: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-05 13:19:56.420206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762330796.428558   50402 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762330796.431020   50402 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1762330796.437629   50402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762330796.437639   50402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762330796.437640   50402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762330796.437641   50402 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-05 13:19:56.439891: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from supplementary.utils.components import GPTModel\n",
    "from supplementary.utils.gpt_generate import load_weights_into_gpt\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"drop_rate\": 0.0,       # Dropout rate\n",
    "    \"qkv_bias\": True        # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb0da2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"gpt2/gpt2-medium-355M.pth\"\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "model.load_state_dict(torch.load(file_name, weights_only=True))\n",
    "model.eval();\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f41f072",
   "metadata": {},
   "source": [
    "- Testing the model on a validation task before finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69de9a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a reponse that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a86e22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from supplementary.utils.gpt_generate import (\n",
    "    generate, text_to_token_ids, token_ids_to_text\n",
    ")\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c511d6",
   "metadata": {},
   "source": [
    "- Compared to previous NBs, where the `generate` function returned both input and output text, we will now need to isolate the response by subtracting the length of the instruction from the start of the `generated_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4980fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Example:\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "### Example:\n"
     ]
    }
   ],
   "source": [
    "response_text = (\n",
    "    generated_text[len(input_text):]\n",
    "    .replace(\"### Reponse:\", \"\")\n",
    "    .strip()\n",
    ")\n",
    "\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cc46fb",
   "metadata": {},
   "source": [
    "- The model isn't capable of following instructions at present. Creating a `Response` section only ends up repeating the original input sentence and the instruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ab7e7",
   "metadata": {},
   "source": [
    "## **6. Finetuning the LLM on Instruction Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330c5a75",
   "metadata": {},
   "source": [
    "- Reusing the loss calculation and training functions in the `components` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4d5d800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from supplementary.utils.components import (\n",
    "    calc_loss_loader,\n",
    "    train_model_simple\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5c802aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.896774673461914\n",
      "Validation loss: 3.8306540966033937\n"
     ]
    }
   ],
   "source": [
    "# Calculating the initial training and validation set loss prior to finetuning the model.abs\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
    "    \n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "659e989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 000000): Train loss 2.730, Val loss 2.706\n",
      "Epoch 1 (Step 000005): Train loss 1.163, Val loss 1.086\n",
      "Epoch 1 (Step 000010): Train loss 0.849, Val loss 0.920\n",
      "Epoch 1 (Step 000015): Train loss 0.846, Val loss 0.894\n",
      "Epoch 1 (Step 000020): Train loss 0.765, Val loss 0.865\n",
      "Epoch 1 (Step 000025): Train loss 0.741, Val loss 0.838\n",
      "Epoch 1 (Step 000030): Train loss 0.786, Val loss 0.820\n",
      "Epoch 1 (Step 000035): Train loss 0.708, Val loss 0.791\n",
      "Epoch 1 (Step 000040): Train loss 0.660, Val loss 0.786\n",
      "Epoch 1 (Step 000045): Train loss 0.626, Val loss 0.775\n",
      "Epoch 1 (Step 000050): Train loss 0.660, Val loss 0.769\n",
      "Epoch 1 (Step 000055): Train loss 0.744, Val loss 0.750\n",
      "Epoch 1 (Step 000060): Train loss 0.710, Val loss 0.732\n",
      "Epoch 1 (Step 000065): Train loss 0.643, Val loss 0.721\n",
      "Epoch 1 (Step 000070): Train loss 0.527, Val loss 0.715\n",
      "Epoch 1 (Step 000075): Train loss 0.560, Val loss 0.715\n",
      "Epoch 1 (Step 000080): Train loss 0.594, Val loss 0.707\n",
      "Epoch 1 (Step 000085): Train loss 0.500, Val loss 0.695\n",
      "Epoch 1 (Step 000090): Train loss 0.554, Val loss 0.678\n",
      "Epoch 1 (Step 000095): Train loss 0.493, Val loss 0.666\n",
      "Epoch 1 (Step 000100): Train loss 0.495, Val loss 0.668\n",
      "Epoch 1 (Step 000105): Train loss 0.560, Val loss 0.661\n",
      "Epoch 1 (Step 000110): Train loss 0.548, Val loss 0.656\n",
      "Epoch 1 (Step 000115): Train loss 0.498, Val loss 0.652\n",
      "Below is an instruction that describes a task. Write a reponse that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes a task. Write a reponse that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: '\n",
      "Epoch 2 (Step 000120): Train loss 0.431, Val loss 0.661\n",
      "Epoch 2 (Step 000125): Train loss 0.442, Val loss 0.678\n",
      "Epoch 2 (Step 000130): Train loss 0.441, Val loss 0.673\n",
      "Epoch 2 (Step 000135): Train loss 0.403, Val loss 0.672\n",
      "Epoch 2 (Step 000140): Train loss 0.405, Val loss 0.668\n",
      "Epoch 2 (Step 000145): Train loss 0.366, Val loss 0.668\n",
      "Epoch 2 (Step 000150): Train loss 0.374, Val loss 0.661\n",
      "Epoch 2 (Step 000155): Train loss 0.404, Val loss 0.660\n",
      "Epoch 2 (Step 000160): Train loss 0.408, Val loss 0.668\n",
      "Epoch 2 (Step 000165): Train loss 0.375, Val loss 0.673\n",
      "Epoch 2 (Step 000170): Train loss 0.323, Val loss 0.669\n",
      "Epoch 2 (Step 000175): Train loss 0.333, Val loss 0.657\n",
      "Epoch 2 (Step 000180): Train loss 0.383, Val loss 0.642\n",
      "Epoch 2 (Step 000185): Train loss 0.409, Val loss 0.641\n",
      "Epoch 2 (Step 000190): Train loss 0.337, Val loss 0.632\n",
      "Epoch 2 (Step 000195): Train loss 0.329, Val loss 0.618\n",
      "Epoch 2 (Step 000200): Train loss 0.300, Val loss 0.616\n",
      "Epoch 2 (Step 000205): Train loss 0.347, Val loss 0.613\n",
      "Epoch 2 (Step 000210): Train loss 0.363, Val loss 0.616\n",
      "Epoch 2 (Step 000215): Train loss 0.394, Val loss 0.619\n",
      "Epoch 2 (Step 000220): Train loss 0.297, Val loss 0.628\n",
      "Epoch 2 (Step 000225): Train loss 0.338, Val loss 0.642\n",
      "Epoch 2 (Step 000230): Train loss 0.289, Val loss 0.638\n",
      "Below is an instruction that describes a task. Write a reponse that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a reponse that appropriately completes the request.  ### Instruction: What is the capital of the United\n",
      "Training completed in 1.98 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    optimizer, \n",
    "    device,\n",
    "    num_epochs=num_epochs, \n",
    "    eval_freq=5, \n",
    "    eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_mins = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_mins:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acd9c71",
   "metadata": {},
   "source": [
    "- Model training progressed as expected with the sample response being printed after every epoch.\n",
    "- The instruction to convert `The chef cooks the meal everyday` into passive voice `The meal is cooked every day by the chef` indicates that the model will now begin to follow instructions more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a9c040d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from supplementary.utils.components import plot_losses\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54376b49",
   "metadata": {},
   "source": [
    "- Overfitting begins to set in around 1 training epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3799a36",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
