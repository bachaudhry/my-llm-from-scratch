{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b854364",
   "metadata": {},
   "source": [
    "# **Attention Mechanisms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7c25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378862a6",
   "metadata": {},
   "source": [
    "## The Need for Attention - In a Nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105764e",
   "metadata": {},
   "source": [
    "As this notebook is being written (mid 2025), the seminal paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) is nearly 8 years old. The abstract states:\n",
    "\n",
    ">The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \n",
    ">\n",
    ">Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \n",
    "\n",
    "Eschewing encoder-decoder RNNS, which were commonly used in translation tasks, in favour of an architecture which could selectively access all input tokens of a given sequence and assign signifance on a comparative basis helped to usher in the age of LLMs as we know them.\n",
    "\n",
    "Transformers built using attention mechanisms addressed the limitations of RNNS i.e.:\n",
    "\n",
    "1. **Short range dependency** i.e. the failure to grasp connections between distant words / sequences.\n",
    "2. **Limited Parallelism** i.e. slow processing of information due to sequential design.\n",
    "3. **Focus on Local Context** i.e. primarily considering immediate neighbours and potentially missing critical information from other parts of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e95b193",
   "metadata": {},
   "source": [
    "## 1. Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3458de",
   "metadata": {},
   "source": [
    "The primary function of self attention is to generate \"context-aware\" vectors from the input sequence.\n",
    "\n",
    "![title](.//images/self-attention.webp)\n",
    "\n",
    "_figure from Deep Learning with Python by Francios Chollet_\n",
    "\n",
    "\n",
    "> The \"self\" refers to the mechanisms ability to compute attention weights by relating different positions within a single input sequence. It assesses and learns the relationships and dependencies between various parts of the input itself, such as words in a sentence or pixels in an image. - S. Raschka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c84b39",
   "metadata": {},
   "source": [
    "### 1.1 Simple Self-Attention Without Trainable Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251daec",
   "metadata": {},
   "source": [
    "The goal is to compute a context vector for each input element that combines information from all other input elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891c5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input tensor representing a sequence which has already by embedded into 3 dimensional vectors\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your    (x^1)\n",
    "     [0.55, 0.87, 0.66], # journey (x^2)\n",
    "     [0.57, 0.85, 0.64], # starts  (x^3)\n",
    "     [0.22, 0.58, 0.33], # with    (x^4)\n",
    "     [0.77, 0.25, 0.10], # one     (x^5)\n",
    "     [0.05, 0.80, 0.55]] # step    (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b7ab735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# Calculate intermediate attention scores by taking dot product of the query with every other input token\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    # dot product without transpose since these are 1 dim vectors\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(attn_scores_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f6eb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n",
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "# Illustration of dot products at work\n",
    "res = 0.\n",
    "\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "\n",
    "print(res)\n",
    "print(torch.dot(inputs[0], query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80b0ce",
   "metadata": {},
   "source": [
    "> Beyond viewing the dot product operations as a mathematical tool...(it) is a measure of similarity because it quantifies how closely two vectors are aligned... In the context of self attention mechanisms, the dot product determines the extent to which each element in a sequence focuses on, or 'attends to' any other element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b6c4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum:  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Simple normalization of the attention scores, they should sum upto ~1.0\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights: \", attn_weights_2_tmp)\n",
    "print(\"Sum: \", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "692ec10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# In practice softmax is used since it is better at handling extreme values and always results in positive values.\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights: \", attn_weights_2_naive)\n",
    "print(\"Sum: \", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e612aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# To correct for numerical instability i.e. over and underflow...stick with the pytorch version which is extensively optimized\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights: \", attn_weights_2)\n",
    "print(\"Sum: \", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8a2cc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# The final step focuses on creating the context vector\n",
    "# Multiply the embedded input tokens with the corresponding attention weights\n",
    "# and then summing.\n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "    \n",
    "print(context_vec_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9844b0",
   "metadata": {},
   "source": [
    "### 1.2 Computing Attention Weights for all Input Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453f782",
   "metadata": {},
   "source": [
    "Let's extend the computation to calculate attention weights and context vectors for all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2abed53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Apply the previous step to all pairwise elements to compute the un-normalized attention score matrix\n",
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs): # additional loop to compute the dot products for all pairs of inputs\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "        \n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dfffb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Replace the for loops with matrix multiplication\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88dee63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# Normalize each row so values sum up to 1\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09eff09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 3 sum: 1.0000001192092896\n",
      "All row sum: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Quick check\n",
    "row_3_sum = attn_weights[2].sum(dim=0)\n",
    "all_row_sum = attn_weights.sum(dim=-1)\n",
    "\n",
    "print(f\"Row 3 sum: {row_3_sum}\")\n",
    "print(f\"All row sum: {all_row_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54569510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Using the attention weights to compute all context vectors via matmul\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e989816d",
   "metadata": {},
   "source": [
    "## 2. Implementing Self Attention With Trainable Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606cb074",
   "metadata": {},
   "source": [
    "With the fundamentals out of the way, it is time to implement the self-attention mechanism used in the original transformer architecture. This self-attention mechanism is also called _scaled dot-product attention_.\n",
    "\n",
    "Context vectors now have to be computed as weighted sums over the input vectors specific to a certain input element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491cdde",
   "metadata": {},
   "source": [
    "### 2.1 Computing the Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76801a8",
   "metadata": {},
   "source": [
    " Let us begin by adding three trainable weight matrices into the mix i.e. $W_{q}$, $W_{k}$ and $W_{v}$. These are used to project the embedded input tokens into query, key and value vectors.\n",
    "\n",
    "- Query vector: $q^{(i)} = x^{(i)}\\,W_q $\n",
    "- Key vector: $k^{(i)} = x^{(i)}\\,W_k $\n",
    "- Value vector: $v^{(i)} = x^{(i)}\\,W_v $\n",
    "\n",
    "For illustration, we can begin by computing only one context vector before moving onto all context vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90d6cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT models have similar input and output dimensions, but we're changing things to understand computations better.\n",
    "\n",
    "x_2 = inputs[1] # 2nd input element\n",
    "d_in = inputs.shape[1] # input embedding size, d=3\n",
    "d_out = 2 # output embedding size, d=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "859ca8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Setting requires_grad=False for cleaner outputs\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db4a8e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0760, 1.7344]) tensor([1.5764, 0.9441]) tensor([1.7073, 1.0646])\n"
     ]
    }
   ],
   "source": [
    "# Computing query, key and value vectors wrt the 2nd input element\n",
    "query_2 = x_2 @ W_query\n",
    "key_2   = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2, key_2, value_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21cdc4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "# 6 input tokens projected from 3d to 2d embedding space\n",
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ace997e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3338)\n"
     ]
    }
   ],
   "source": [
    "# Now, computing the un-normalized attention scores by taking the dot product of the query and each\n",
    "# key vector\n",
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(key_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf25e3",
   "metadata": {},
   "source": [
    "With regard to attention scores, the difference from the previous section is that instead of directly computing the dot-product between the input elements, we will use the query and key obtained by transforming the inputs via respective weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7389a476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.7084, 3.3338, 3.3013, 1.7563, 1.7869, 2.1966])\n"
     ]
    }
   ],
   "source": [
    "# Generalizing to all attention scores via matrix multiplications\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a520628",
   "metadata": {},
   "source": [
    "Attention weights are computed by scaling the attention scores and using softmax. But, at this point, we can scale the attention scores by dividing them by the square root of the embedding dimension of the keys, which is the mathematically similar to exponentiating by 0.5.\n",
    "\n",
    "Normalizing by the embedding dimension size improves training performance by avoiding small gradients caused by the usage of softmax when scaling up.\n",
    "\n",
    "> As dot products increase, the softmax function behaves like a step function, resulting in gradients nearing zero...Scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "476018e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1723, 0.2681, 0.2620, 0.0879, 0.0898, 0.1200])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77749c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4201, 0.8892])\n"
     ]
    }
   ],
   "source": [
    "# Computing context vector\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6c856",
   "metadata": {},
   "source": [
    "### 2.2 Creating a Compact SelfAttention Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14256d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3751, 0.8610],\n",
      "        [1.4201, 0.8892],\n",
      "        [1.4198, 0.8890],\n",
      "        [1.3533, 0.8476],\n",
      "        [1.3746, 0.8606],\n",
      "        [1.3620, 0.8532]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec        \n",
    "    \n",
    "\n",
    "torch.manual_seed(42)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10781397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4201, 0.8892])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Quick check\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9a7d96",
   "metadata": {},
   "source": [
    "The Self-Attention class can make use of `nn.Linear` layers to carry out matrix multiplications (especially when bias units have been disabled). These layers also have optimized weight initialization schemes which contribute to more stable training. Consequently, when compared, `SelfAttention_v1` and `SelfAttention_v2` will have differing outputs due to the differences in weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12af88ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3755, 0.2777],\n",
      "        [0.3761, 0.2831],\n",
      "        [0.3761, 0.2833],\n",
      "        [0.3768, 0.2763],\n",
      "        [0.3754, 0.2836],\n",
      "        [0.3772, 0.2746]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys =  self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "    \n",
    "torch.manual_seed(42)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c239668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3755, 0.2777],\n",
       "        [0.3761, 0.2831],\n",
       "        [0.3761, 0.2833],\n",
       "        [0.3768, 0.2763],\n",
       "        [0.3754, 0.2836],\n",
       "        [0.3772, 0.2746]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise - Assigning the weights from an instance of SelfAttention__v2 to an instance of v1\n",
    "sa_v1.W_key = torch.nn.Parameter(sa_v2.W_key.weight.T)\n",
    "sa_v1.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)\n",
    "sa_v1.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)\n",
    "\n",
    "sa_v1(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7453739d",
   "metadata": {},
   "source": [
    "## 3. Implementing Causal Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71b1c59",
   "metadata": {},
   "source": [
    "Causal or _masked_ attention restricts the model to only consider previous and current inputs in a sequence when processing any given input token's attention scores. Attention weights \"above the diagonal\" are masked out and non-masked attention weights are normalized so that they sum to 1. in each row.\n",
    "\n",
    "![title](.//images/causal-self-attention.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e04c45a",
   "metadata": {},
   "source": [
    "To apply this mask the previous self-attention mechanism will be converted to causal self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37c757c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1605, 0.1726, 0.1714, 0.1681, 0.1473, 0.1801],\n",
      "        [0.1627, 0.1780, 0.1758, 0.1648, 0.1306, 0.1880],\n",
      "        [0.1625, 0.1782, 0.1759, 0.1648, 0.1302, 0.1885],\n",
      "        [0.1661, 0.1726, 0.1715, 0.1654, 0.1475, 0.1768],\n",
      "        [0.1596, 0.1777, 0.1755, 0.1664, 0.1312, 0.1896],\n",
      "        [0.1682, 0.1715, 0.1707, 0.1648, 0.1511, 0.1738]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# We shall reuse the query and key weights of SelfAttention_v2\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab8fe846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Pytorch's tril function is a simple way to create a mask.\n",
    "# Here values above diagonal are zero\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17702ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1605, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1627, 0.1780, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1625, 0.1782, 0.1759, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1661, 0.1726, 0.1715, 0.1654, 0.0000, 0.0000],\n",
      "        [0.1596, 0.1777, 0.1755, 0.1664, 0.1312, 0.0000],\n",
      "        [0.1682, 0.1715, 0.1707, 0.1648, 0.1511, 0.1738]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Multiplying the mask with the attention weights to zero out the values above the diagonal\n",
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029d111",
   "metadata": {},
   "source": [
    "It should be noted that applying the mask after normalizing with softmax would disrupt the probability distribution of the initial softmax norm.\n",
    "\n",
    "When negative `inf` values are present in a row, the softmax function treats them as zero probability, since $e^{-inf}$ approaches zero. So we can create a more effective mask by replacing zeros above the diagonal with $-inf$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3bad797b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0508,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.2157,  0.3428,    -inf,    -inf,    -inf,    -inf],\n",
      "        [ 0.2163,  0.3467,  0.3282,    -inf,    -inf,    -inf],\n",
      "        [ 0.1257,  0.1799,  0.1707,  0.1191,    -inf,    -inf],\n",
      "        [ 0.1667,  0.3193,  0.3012,  0.2258, -0.1098,    -inf],\n",
      "        [ 0.1269,  0.1548,  0.1475,  0.0978, -0.0247,  0.1731]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1) # insert 1s above the diagonal\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf) # swap with -inf\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "177a65a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4775, 0.5225, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3146, 0.3450, 0.3405, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2459, 0.2555, 0.2538, 0.2448, 0.0000, 0.0000],\n",
      "        [0.1969, 0.2193, 0.2165, 0.2053, 0.1619, 0.0000],\n",
      "        [0.1682, 0.1715, 0.1707, 0.1648, 0.1511, 0.1738]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the updated masked attention weights\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ecd23",
   "metadata": {},
   "source": [
    "### 3.1 Masking Attention Weights with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094cbd78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dca8e1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 0., 2.],\n",
      "        [0., 0., 2., 2., 2., 2.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [0., 2., 2., 0., 2., 2.],\n",
      "        [2., 2., 0., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dropout = torch.nn.Dropout(0.5) # Dropout rate of 50% for illustration\n",
    "example = torch.ones(6, 6)\n",
    "\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb83e44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6809, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.5110, 0.5077, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3938, 0.4387, 0.0000, 0.4106, 0.3239, 0.0000],\n",
      "        [0.3364, 0.3431, 0.3413, 0.3295, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Doing the same for the attention weights we calculated in the previous section\n",
    "torch.manual_seed(42)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5091aee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4775, 0.5225, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3146, 0.3450, 0.3405, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2459, 0.2555, 0.2538, 0.2448, 0.0000, 0.0000],\n",
       "        [0.1969, 0.2193, 0.2165, 0.2053, 0.1619, 0.0000],\n",
       "        [0.1682, 0.1715, 0.1707, 0.1648, 0.1511, 0.1738]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a07b0d",
   "metadata": {},
   "source": [
    "With a 50% dropout rate, the values of the remaining elements are scaled up by a factor of `1 / 0.5 = 2`, which is crucial to maintain the overall balance of the attention weights and the average attention mechanism remains consistent during training and inference.\n",
    "\n",
    "Numerical values of the tensor passed to `nn.Dropout()` produces different ouputs on different OS'. This is currently an open issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01758613",
   "metadata": {},
   "source": [
    "### 3.2 Implementing a Compact Causal Self-Attention Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383c57a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d4b51328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "# This results in batch size of 2 with 6 tokens each and each token has embedding dimension of 3\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a69ee425",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out   = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # b is the batch dim\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # num_tokens exceeding contenxt_length will result in errors during masking\n",
    "        keys    = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values  = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_scores.masked_fill(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens],\n",
    "            -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c003665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3755, 0.2777],\n",
      "         [0.3761, 0.2831],\n",
      "         [0.3761, 0.2833],\n",
      "         [0.3768, 0.2763],\n",
      "         [0.3754, 0.2836],\n",
      "         [0.3772, 0.2746]],\n",
      "\n",
      "        [[0.3755, 0.2777],\n",
      "         [0.3761, 0.2831],\n",
      "         [0.3761, 0.2833],\n",
      "         [0.3768, 0.2763],\n",
      "         [0.3754, 0.2836],\n",
      "         [0.3772, 0.2746]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a643b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5627, 0.4484],\n",
      "         [0.3605, 0.3069],\n",
      "         [0.5189, 0.3813],\n",
      "         [0.7535, 0.5526],\n",
      "         [0.3299, 0.2006],\n",
      "         [0.5052, 0.2738]],\n",
      "\n",
      "        [[0.3351, 0.2701],\n",
      "         [0.4826, 0.4155],\n",
      "         [0.4810, 0.4097],\n",
      "         [0.3352, 0.2702],\n",
      "         [0.3169, 0.1386],\n",
      "         [0.3387, 0.1419]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.5) # Testing 50% dropout rate.\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb431e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
