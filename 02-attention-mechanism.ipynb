{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b854364",
   "metadata": {},
   "source": [
    "# **Attention Mechanisms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7c25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378862a6",
   "metadata": {},
   "source": [
    "## The Need for Attention - In a Nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105764e",
   "metadata": {},
   "source": [
    "As this notebook is being written (mid 2025), the seminal paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) is nearly 8 years old. The abstract states:\n",
    "\n",
    ">The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \n",
    ">\n",
    ">Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \n",
    "\n",
    "Eschewing encoder-decoder RNNS, which were commonly used in translation tasks, in favour of an architecture which could selectively access all input tokens of a given sequence and assign signifance on a comparative basis helped to usher in the age of LLMs as we know them.\n",
    "\n",
    "Transformers built using attention mechanisms addressed the limitations of RNNS i.e.:\n",
    "\n",
    "1. **Short range dependency** i.e. the failure to grasp connections between distant words / sequences.\n",
    "2. **Limited Parallelism** i.e. slow processing of information due to sequential design.\n",
    "3. **Focus on Local Context** i.e. primarily considering immediate neighbours and potentially missing critical information from other parts of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e95b193",
   "metadata": {},
   "source": [
    "## 1. Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3458de",
   "metadata": {},
   "source": [
    "The primary function of self attention is to generate \"context-aware\" vectors from the input sequence.\n",
    "\n",
    "![title](.//images/self-attention.webp)\n",
    "\n",
    "_figure from Deep Learning with Python by Francios Chollet_\n",
    "\n",
    "\n",
    "> The \"self\" refers to the mechanisms ability to compute attention weights by relating different positions within a single input sequence. It assesses and learns the relationships and dependencies between various parts of the input itself, such as words in a sentence or pixels in an image. - S. Raschka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c84b39",
   "metadata": {},
   "source": [
    "### 1.1 Simple Self-Attention Without Trainable Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251daec",
   "metadata": {},
   "source": [
    "The goal is to compute a context vector for each input element that combines information from all other input elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891c5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input tensor representing a sequence which has already by embedded into 3 dimensional vectors\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your    (x^1)\n",
    "     [0.55, 0.87, 0.66], # journey (x^2)\n",
    "     [0.57, 0.85, 0.64], # starts  (x^3)\n",
    "     [0.22, 0.58, 0.33], # with    (x^4)\n",
    "     [0.77, 0.25, 0.10], # one     (x^5)\n",
    "     [0.05, 0.80, 0.55]] # step    (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ab735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# Calculate intermediate attention scores by taking dot product of the query with every other input token\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    # dot product without transpose since these are 1 dim vectors\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(attn_scores_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f6eb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n",
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "# Illustration of dot products at work\n",
    "res = 0.\n",
    "\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "\n",
    "print(res)\n",
    "print(torch.dot(inputs[0], query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80b0ce",
   "metadata": {},
   "source": [
    "> Beyond viewing the dot product operations as a mathematical tool...(it) is a measure of similarity because it quantifies how closely two vectors are aligned... In the context of self attention mechanisms, the dot product determines the extent to which each element in a sequence focuses on, or 'attends to' any other element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b6c4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum:  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Simple normalization of the attention scores, they should sum upto ~1.0\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights: \", attn_weights_2_tmp)\n",
    "print(\"Sum: \", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "692ec10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# In practice softmax is used since it is better at handling extreme values and always results in positive values.\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights: \", attn_weights_2_naive)\n",
    "print(\"Sum: \", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30295e30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e612aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# To correct for numerical instability i.e. over and underflow...stick with the pytorch version which is extensively optimized\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights: \", attn_weights_2)\n",
    "print(\"Sum: \", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8a2cc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# The final step focuses on creating the context vector\n",
    "# Multiply the embedded input tokens with the corresponding attention weights\n",
    "# and then summing.\n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "    \n",
    "print(context_vec_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9844b0",
   "metadata": {},
   "source": [
    "### 1.2 Computing Attention Weights for all Input Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453f782",
   "metadata": {},
   "source": [
    "Let's extend the computation to calculate attention weights and context vectors for all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2abed53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Apply the previous step to all pairwise elements to compute the un-normalized attention score matrix\n",
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs): # additional loop to compute the dot products for all pairs of inputs\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "        \n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dfffb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Replace the for loops with matrix multiplication\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88dee63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# Normalize each row so values sum up to 1\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09eff09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 3 sum: 1.0000001192092896\n",
      "All row sum: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Quick check\n",
    "row_3_sum = attn_weights[2].sum(dim=0)\n",
    "all_row_sum = attn_weights.sum(dim=-1)\n",
    "\n",
    "print(f\"Row 3 sum: {row_3_sum}\")\n",
    "print(f\"All row sum: {all_row_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54569510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Using the attention weights to compute all context vectors via matmul\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e989816d",
   "metadata": {},
   "source": [
    "## 3. Implementing Self Attention With Trainable Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606cb074",
   "metadata": {},
   "source": [
    "With the fundamentals out of the way, it is time to implement the self-attention mechanism used in the original transformer architecture. This self-attention mechanism is also called _scaled dot-product attention_.\n",
    "\n",
    "Context vectors now have to be computed as weighted sums over the input vectors specific to a certain input element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491cdde",
   "metadata": {},
   "source": [
    "### 3.1 Computing the Attention Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76801a8",
   "metadata": {},
   "source": [
    " Let us begin by adding three trainable weight matrices into the mix i.e. $W_{q}$, $W_{k}$ and $W_{v}$. These are used to project the embedded input tokens into query, key and value vectors.\n",
    "\n",
    "- Query vector: $q^{(i)} = x^{(i)}\\,W_q $\n",
    "- Key vector: $k^{(i)} = x^{(i)}\\,W_k $\n",
    "- Value vector: $v^{(i)} = x^{(i)}\\,W_v $\n",
    "\n",
    "For illustration, we can begin by computing only one context vector before moving onto all context vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90d6cf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT models have similar input and output dimensions, but we're changing things to understand computations better.\n",
    "\n",
    "x_2 = inputs[1] # 2nd input element\n",
    "d_in = inputs.shape[1] # input embedding size, d=3\n",
    "d_out = 2 # output embedding size, d=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "859ca8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Setting requires_grad=False for cleaner outputs\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db4a8e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0760, 1.7344]) tensor([1.5764, 0.9441]) tensor([1.7073, 1.0646])\n"
     ]
    }
   ],
   "source": [
    "# Computing query, key and value vectors wrt the 2nd input element\n",
    "query_2 = x_2 @ W_query\n",
    "key_2   = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2, key_2, value_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21cdc4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "# 6 input tokens projected from 3d to 2d embedding space\n",
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ace997e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3338)\n"
     ]
    }
   ],
   "source": [
    "# Now, computing the un-normalized attention scores by taking the dot product of the query and each\n",
    "# key vector\n",
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(key_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf25e3",
   "metadata": {},
   "source": [
    "With regard to attention scores, the difference from the previous section is that instead of directly computing the dot-product between the input elements, we will use the query and key obtained by transforming the inputs via respective weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7389a476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.7084, 3.3338, 3.3013, 1.7563, 1.7869, 2.1966])\n"
     ]
    }
   ],
   "source": [
    "# Generalizing to all attention scores via matrix multiplications\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a520628",
   "metadata": {},
   "source": [
    "Attention weights are computed by scaling the attention scores and using softmax. But, at this point, we can scale the attention scores by dividing them by the square root of the embedding dimension of the keys, which is the mathematically similar to exponentiating by 0.5.\n",
    "\n",
    "Normalizing by the embedding dimension size improves training performance by avoiding small gradients caused by the usage of softmax when scaling up.\n",
    "\n",
    "> As dot products increase, the softmax function behaves like a step function, resulting in gradients nearing zero...Scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "476018e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1723, 0.2681, 0.2620, 0.0879, 0.0898, 0.1200])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "77749c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4201, 0.8892])\n"
     ]
    }
   ],
   "source": [
    "# Computing context vector\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14256d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
