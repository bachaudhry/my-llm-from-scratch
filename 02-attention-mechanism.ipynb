{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b854364",
   "metadata": {},
   "source": [
    "# **Attention Mechanisms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7c25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378862a6",
   "metadata": {},
   "source": [
    "## The Need for Attention - In a Nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105764e",
   "metadata": {},
   "source": [
    "As this notebook is being written (mid 2025), the seminal paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) is nearly 8 years old. The abstract states:\n",
    "\n",
    ">The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \n",
    ">\n",
    ">Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \n",
    "\n",
    "Eschewing encoder-decoder RNNS, which were commonly used in translation tasks, in favour of an architecture which could selectively access all input tokens of a given sequence and assign signifance on a comparative basis helped to usher in the age of LLMs as we know them.\n",
    "\n",
    "Transformers built using attention mechanisms addressed the limitations of RNNS i.e.:\n",
    "\n",
    "1. **Short range dependency** i.e. the failure to grasp connections between distant words / sequences.\n",
    "2. **Limited Parallelism** i.e. slow processing of information due to sequential design.\n",
    "3. **Focus on Local Context** i.e. primarily considering immediate neighbours and potentially missing critical information from other parts of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e95b193",
   "metadata": {},
   "source": [
    "## 1. Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3458de",
   "metadata": {},
   "source": [
    "The primary function of self attention is to generate \"context-aware\" vectors from the input sequence.\n",
    "\n",
    "![title](.//images/self-attention.webp)\n",
    "\n",
    "_figure from Deep Learning with Python by Francios Chollet_\n",
    "\n",
    "\n",
    "> The \"self\" refers to the mechanisms ability to compute attention weights by relating different positions within a single input sequence. It assesses and learns the relationships and dependencies between various parts of the input itself, such as words in a sentence or pixels in an image. - S. Raschka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c84b39",
   "metadata": {},
   "source": [
    "### 1.1 Simple Self-Attention Without Trainable Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251daec",
   "metadata": {},
   "source": [
    "The goal is to compute a context vector for each input element that combines information from all other input elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891c5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input tensor representing a sequence which has already by embedded into 3 dimensional vectors\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your    (x^1)\n",
    "     [0.55, 0.87, 0.66], # journey (x^2)\n",
    "     [0.57, 0.85, 0.64], # starts  (x^3)\n",
    "     [0.22, 0.58, 0.33], # with    (x^4)\n",
    "     [0.77, 0.25, 0.10], # one     (x^5)\n",
    "     [0.05, 0.80, 0.55]] # step    (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ab735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# Calculate intermediate attention scores by taking dot product of the query with every other input token\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    # dot product without transpose since these are 1 dim vectors\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(attn_scores_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f6eb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n",
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "# Illustration of dot products at work\n",
    "res = 0.\n",
    "\n",
    "for idx, element in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "\n",
    "print(res)\n",
    "print(torch.dot(inputs[0], query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80b0ce",
   "metadata": {},
   "source": [
    "> Beyond viewing the dot product operations as a mathematical tool...(it) is a measure of similarity because it quantifies how closely two vectors are aligned... In the context of self attention mechanisms, the dot product determines the extent to which each element in a sequence focuses on, or 'attends to' any other element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b6c4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum:  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Simple normalization of the attention scores, they should sum upto ~1.0\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights: \", attn_weights_2_tmp)\n",
    "print(\"Sum: \", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "692ec10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# In practice softmax is used since it is better at handling extreme values and always results in positive values.\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights: \", attn_weights_2_naive)\n",
    "print(\"Sum: \", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30295e30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e612aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# To correct for numerical instability i.e. over and underflow...stick with the pytorch version which is extensively optimized\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights: \", attn_weights_2)\n",
    "print(\"Sum: \", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8a2cc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# The final step focuses on creating the context vector\n",
    "# Multiply the embedded input tokens with the corresponding attention weights\n",
    "# and then summing.\n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "    \n",
    "print(context_vec_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9844b0",
   "metadata": {},
   "source": [
    "### 1.2 Computing Attention Weights for all Input Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453f782",
   "metadata": {},
   "source": [
    "Let's extend the computation to calculate attention weights and context vectors for all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2abed53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Apply the previous step to all pairwise elements to compute the un-normalized attention score matrix\n",
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs): # additional loop to compute the dot products for all pairs of inputs\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "        \n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dfffb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Replace the for loops with matrix multiplication\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88dee63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "# Normalize each row so values sum up to 1\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09eff09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 3 sum: 1.0000001192092896\n",
      "All row sum: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Quick check\n",
    "row_3_sum = attn_weights[2].sum(dim=0)\n",
    "all_row_sum = attn_weights.sum(dim=-1)\n",
    "\n",
    "print(f\"Row 3 sum: {row_3_sum}\")\n",
    "print(f\"All row sum: {all_row_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54569510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# Using the attention weights to compute all context vectors via matmul\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e989816d",
   "metadata": {},
   "source": [
    "## 3. Implementing Self Attention With Trainable Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2491cdde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76801a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d6cf40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859ca8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a8e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cdc4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace997e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
