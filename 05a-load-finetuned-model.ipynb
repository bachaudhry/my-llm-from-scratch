{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea8b08b",
   "metadata": {},
   "source": [
    "# Loading and Using Finetuned Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774716a5",
   "metadata": {},
   "source": [
    "This notebook contains simplified code to load a fine-tuned classification model, which in this case is based on GPT-2 Medium (355M). The detailed code can be found in [Notebook 05-finetuning-text-classification.ipynb](05-finetuning-text-classification.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da0df212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n",
      "torch version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"tiktoken\",\n",
    "    \"torch\"\n",
    "]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0f57b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ft_model_path = Path(\"models/review_classifier_gpt2_medium.pth\")\n",
    "\n",
    "if not ft_model_path.exists():\n",
    "    print(f\"Couldn't find {ft_model_path}.\\n\" \n",
    "           \"Re-run NB 05-finetuning-text-classification to fine tune and save a model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c477ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.components import GPTModel\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "# Initialize \n",
    "model = GPTModel(BASE_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b325f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "# Convert model to classifier \n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)\n",
    "\n",
    "# Load the pretrained weights \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(ft_model_path, map_location=device, weights_only=True))\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3990d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48e157bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify a review\n",
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare inputs to the model\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[0]\n",
    "    \n",
    "    # Truncate sequences if they are too long\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "    \n",
    "    # Pad sequences to the longest sequence\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0) # added batch dim\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor.to(device))[:, -1, :] # Logits of the last output token\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    # Return classified result\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cd1eced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "text_1 = (\n",
    "    \"Narrated by Liam Neeson, Everest follows a daring team of mountaineers on a quest to summit Earthâ€™s highest peak.\"\n",
    "    \" Captured with astonishing clarity, this awe-inspiring film takes you deep into the majesty of the Himalayas.\"\n",
    ")\n",
    "\n",
    "print(classify_review(text_1, model, tokenizer, device, max_length=120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "907405aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not spam\n"
     ]
    }
   ],
   "source": [
    "text_2 = (\n",
    "    \"Hi Bilal, I have attached the rental ledger. \" \n",
    "    \" We would like to start advertising this property from now.\"\n",
    ")\n",
    "\n",
    "print(classify_review(text_2, model, tokenizer, device, max_length=120))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c2a69a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
