{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a649c886",
   "metadata": {},
   "source": [
    "# **Create The Byte-Pair Encoding (BPE) Tokenizer From Scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38496ca",
   "metadata": {},
   "source": [
    "As of 2025, BPE is still popular and is widely used. Models including GPT-2, GPT-3, GPT-4, Llama-3 etc. have made use fo this tokenizer. \n",
    "\n",
    "OpenAI's original implementation of the BPE tokenizer can be found [here](https://github.com/openai/gpt-2/blob/master/src/encoder.py), while practitioners usually incorporate the [tiktoken](https://github.com/openai/tiktoken) library in their model development pipelines. Karpathy's [minBPE](https://github.com/karpathy/minbpe) is also mentioned in Sebastian's work, as a possible alternative to the worflow below.\n",
    "\n",
    "For practice, the BPE tokenizer will be implemented from scratch in this notebook - though it won't be nearly as optimized as OpenAI's or maybe even Karpathy's versions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046b45e",
   "metadata": {},
   "source": [
    "## **BPE Algorithm Outline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b72c76",
   "metadata": {},
   "source": [
    ">**1. Identify frequent pairs**\n",
    ">- In each iteration, scan the text to find the most commonly occurring pair of bytes (or characters)\n",
    ">\n",
    ">**2. Replace and record**\n",
    ">\n",
    ">- Replace that pair with a new placeholder ID (one not already in use, e.g., if we start with 0...255, the first placeholder would be 256)\n",
    ">- Record this mapping in a lookup table\n",
    ">- The size of the lookup table is a hyperparameter, also called \"vocabulary size\" (for GPT-2, that's\n",
    ">50,257)\n",
    ">\n",
    ">**3. Repeat until no gains**\n",
    ">\n",
    ">- Keep repeating steps 1 and 2, continually merging the most frequent pairs\n",
    ">- Stop when no further compression is possible (e.g., no pair occurs more than once)\n",
    ">\n",
    ">**Decompression (decoding)**\n",
    ">\n",
    ">- To restore the original text, reverse the process by substituting each ID with its corresponding pair, using the lookup table\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b8eab",
   "metadata": {},
   "source": [
    "### **Working Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2fc3cd",
   "metadata": {},
   "source": [
    ">&nbsp;\n",
    "> Suppose we have the text (training dataset) `the cat in the hat` from which we want to build the vocabulary for a BPE tokenizer\n",
    ">\n",
    ">**Iteration 1**\n",
    ">\n",
    ">1. Identify frequent pairs\n",
    ">  - In this text, \"th\" appears twice (at the beginning and before the second \"e\")\n",
    ">\n",
    ">2. Replace and record\n",
    ">  - replace \"th\" with a new token ID that is not already in use, e.g., 256\n",
    ">  - the new text is: `<256>e cat in <256>e hat`\n",
    ">  - the new vocabulary is\n",
    ">\n",
    ">```\n",
    ">  0: ...\n",
    ">  ...\n",
    ">  256: \"th\"\n",
    ">```\n",
    ">\n",
    ">**Iteration 2**\n",
    ">\n",
    ">1. **Identify frequent pairs**  \n",
    ">   - In the text `<256>e cat in <256>e hat`, the pair `<256>e` appears twice\n",
    ">\n",
    ">2. **Replace and record**  \n",
    ">   - replace `<256>e` with a new token ID that is not already in use, for example, `257`.  \n",
    ">   - The new text is:\n",
    ">     ```\n",
    ">     <257> cat in <257> hat\n",
    ">     ```\n",
    ">   - The updated vocabulary is:\n",
    ">     ```\n",
    ">     0: ...\n",
    ">     ...\n",
    ">     256: \"th\"\n",
    ">     257: \"<256>e\"\n",
    ">     ```\n",
    ">\n",
    ">**Iteration 3**\n",
    ">\n",
    ">1. **Identify frequent pairs**  \n",
    ">   - In the text `<257> cat in <257> hat`, the pair `<257> ` appears twice (once at the beginning and once before “hat”).\n",
    ">\n",
    ">2. **Replace and record**  \n",
    ">   - replace `<257> ` with a new token ID that is not already in use, for example, `258`.  \n",
    ">   - the new text is:\n",
    ">     ```\n",
    ">     <258>cat in <258>hat\n",
    ">     ```\n",
    ">   - The updated vocabulary is:\n",
    ">     ```\n",
    ">     0: ...\n",
    ">     ...\n",
    ">     256: \"th\"\n",
    ">     257: \"<256>e\"\n",
    ">     258: \"<257> \"\n",
    ">     ```\n",
    ">     \n",
    ">- and so forth\n",
    ">\n",
    ">&nbsp;\n",
    ">#### Decoding Steps:\n",
    ">\n",
    ">- To restore the original text, we reverse the process by substituting each token ID with its corresponding pair in the reverse order they were introduced\n",
    ">- Start with the final compressed text: `<258>cat in <258>hat`\n",
    ">-  Substitute `<258>` → `<257> `: `<257> cat in <257> hat`  \n",
    ">- Substitute `<257>` → `<256>e`: `<256>e cat in <256>e hat`\n",
    ">- Substitute `<256>` → \"th\": `the cat in the hat`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075aad61",
   "metadata": {},
   "source": [
    "## **Simplified BPE Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f512f4",
   "metadata": {},
   "source": [
    "This is a simplified implementation of the BPE algorithm, which will mimic the `tiktoken` UI. Here the `encode()` method will approximate the original `train()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2cafa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c2083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m lru_cache(maxsize=\u001b[32m128\u001b[39m, typed=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mSource:\u001b[39m   \n",
      "\u001b[38;5;28;01mdef\u001b[39;00m lru_cache(maxsize=\u001b[32m128\u001b[39m, typed=\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "    \u001b[33m\"\"\"Least-recently-used cache decorator.\u001b[39m\n",
      "\n",
      "\u001b[33m    If *maxsize* is set to None, the LRU features are disabled and the cache\u001b[39m\n",
      "\u001b[33m    can grow without bound.\u001b[39m\n",
      "\n",
      "\u001b[33m    If *typed* is True, arguments of different types will be cached separately.\u001b[39m\n",
      "\u001b[33m    For example, f(3.0) and f(3) will be treated as distinct calls with\u001b[39m\n",
      "\u001b[33m    distinct results.\u001b[39m\n",
      "\n",
      "\u001b[33m    Arguments to the cached function must be hashable.\u001b[39m\n",
      "\n",
      "\u001b[33m    View the cache statistics named tuple (hits, misses, maxsize, currsize)\u001b[39m\n",
      "\u001b[33m    with f.cache_info().  Clear the cache and statistics with f.cache_clear().\u001b[39m\n",
      "\u001b[33m    Access the underlying function with f.__wrapped__.\u001b[39m\n",
      "\n",
      "\u001b[33m    See:  https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)\u001b[39m\n",
      "\n",
      "\u001b[33m    \"\"\"\u001b[39m\n",
      "\n",
      "    \u001b[38;5;66;03m# Users should only access the lru_cache through its public API:\u001b[39;00m\n",
      "    \u001b[38;5;66;03m#       cache_info, cache_clear, and f.__wrapped__\u001b[39;00m\n",
      "    \u001b[38;5;66;03m# The internals of the lru_cache are encapsulated for thread safety and\u001b[39;00m\n",
      "    \u001b[38;5;66;03m# to allow the implementation to change (including a possible C version).\u001b[39;00m\n",
      "\n",
      "    \u001b[38;5;28;01mif\u001b[39;00m isinstance(maxsize, int):\n",
      "        \u001b[38;5;66;03m# Negative maxsize is treated as 0\u001b[39;00m\n",
      "        \u001b[38;5;28;01mif\u001b[39;00m maxsize < \u001b[32m0\u001b[39m:\n",
      "            maxsize = \u001b[32m0\u001b[39m\n",
      "    \u001b[38;5;28;01melif\u001b[39;00m callable(maxsize) \u001b[38;5;28;01mand\u001b[39;00m isinstance(typed, bool):\n",
      "        \u001b[38;5;66;03m# The user_function was passed in directly via the maxsize argument\u001b[39;00m\n",
      "        user_function, maxsize = maxsize, \u001b[32m128\u001b[39m\n",
      "        wrapper = _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo)\n",
      "        wrapper.cache_parameters = \u001b[38;5;28;01mlambda\u001b[39;00m : {\u001b[33m'maxsize'\u001b[39m: maxsize, \u001b[33m'typed'\u001b[39m: typed}\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m update_wrapper(wrapper, user_function)\n",
      "    \u001b[38;5;28;01melif\u001b[39;00m maxsize \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "        \u001b[38;5;28;01mraise\u001b[39;00m TypeError(\n",
      "            \u001b[33m'Expected first argument to be an integer, a callable, or None'\u001b[39m)\n",
      "\n",
      "    \u001b[38;5;28;01mdef\u001b[39;00m decorating_function(user_function):\n",
      "        wrapper = _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo)\n",
      "        wrapper.cache_parameters = \u001b[38;5;28;01mlambda\u001b[39;00m : {\u001b[33m'maxsize'\u001b[39m: maxsize, \u001b[33m'typed'\u001b[39m: typed}\n",
      "        \u001b[38;5;28;01mreturn\u001b[39;00m update_wrapper(wrapper, user_function)\n",
      "\n",
      "    \u001b[38;5;28;01mreturn\u001b[39;00m decorating_function\n",
      "\u001b[31mFile:\u001b[39m      ~/.local/share/uv/python/cpython-3.11.12-linux-x86_64-gnu/lib/python3.11/functools.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "class BPETokenizerLocal:\n",
    "    def __init__(self):\n",
    "        # Map token_id to token_str   \n",
    "        self.vocab = {}\n",
    "        # Map token_str to token_od\n",
    "        self.inverse_vocab = {}\n",
    "        # Use a rank dict for GPT-2 merges. Low ranks have higher priority\n",
    "        self.bpe_ranks = {}\n",
    "     \n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        Train BPE tokenizer from scratch\n",
    "\n",
    "        Args:\n",
    "            text (str): Input / training text\n",
    "            vocab_size (int): Desired vocabulary size\n",
    "            allowed_special (set): Set of special tokens to include.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Preprocessing: Replace spaces with \"Ġ\", as implemented in GPT-2.\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "        \n",
    "        # Initialize vocab with unique characters, including \"Ġ\" if present starting\n",
    "        # with first 256 ASCII characters\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "        unique_chars.extend(\n",
    "            char for char in sorted(set(processed_text))\n",
    "            if char not in unique_chars\n",
    "        )\n",
    "        if \"Ġ\" not in unique_chars:\n",
    "            unique_chars.append(\"Ġ\")\n",
    "            \n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "        \n",
    "        # Add allowed special tokens\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "        \n",
    "        # Tokenize the processed_text into token Ids\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "        \n",
    "        # BPE steps: Repeatedly find and replace frequent pairs\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode='most')\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "            \n",
    "        # Build vocab with merged tokens\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "            \n",
    "    def load_vocab_and_merges_from_openai(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load pretained vocabulary and BPE merges from OpenAI's GPT-2 files\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocab file (GPT-2 calls it 'encoder.json)\n",
    "            bpe_merges_path (str): Path to bpe_merges file (GPT-2 calls it 'vocab.bpe'). \n",
    "        \"\"\"\n",
    "        # Load vocab\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            # Load vocab to correct format\n",
    "            self.vocab = {int(v): k for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {k: int(v) for k, v in loaded_vocab.items()}\n",
    "        \n",
    "        # Handle newline character without adding a new token\n",
    "        if \"\\n\" not in self.inverse_vocab:\n",
    "            # Use existing token ID as a placeholder for '\\n' i.e. \"<|endoftext|>\" if available\n",
    "            fallback_token = next((token for token in [\"<|endoftext|>\", \"Ġ\", \"\"] if token in self.inverse_vocab), None)\n",
    "            if fallback_token is not None:\n",
    "                newline_token_id = self.inverse_vocab[fallback_token]\n",
    "            else:\n",
    "                raise KeyError(\"No suitable token found in vocabulary to map '\\\\n'.\")\n",
    "            \n",
    "            self.inverse_vocab[\"\\n\"] = newline_token_id\n",
    "            self.vocab[newline_token_id]= \"\\n\"\n",
    "            \n",
    "        # Load GPT-2 merges and store these with an assigned rank.\n",
    "        self.bpe_ranks = {}\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            if lines and lines[0].startswith(\"#\"):\n",
    "                lines = lines[1:]\n",
    "            \n",
    "            rank = 0\n",
    "            for line in lines:\n",
    "                pair = tuple(line.strip().split())\n",
    "                if len(pair) == 2:\n",
    "                    token1, token2 = pair\n",
    "                    # if both tokens are not in vocab then skip\n",
    "                    if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
    "                        self.bpe_ranks[(token1, token2)] = rank\n",
    "                        rank += 1\n",
    "                    else:\n",
    "                        print(f\"Skipping pair {pair} since one token isn't in the vocabulary!\")\n",
    "                        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf63b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07ff6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbacaf1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13665215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c626c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c775399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9628b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251fde90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
