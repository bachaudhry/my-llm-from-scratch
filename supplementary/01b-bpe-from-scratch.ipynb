{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a649c886",
   "metadata": {},
   "source": [
    "# **Create The Byte-Pair Encoding (BPE) Tokenizer From Scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38496ca",
   "metadata": {},
   "source": [
    "As of 2025, BPE is still popular and is widely used. Models including GPT-2, GPT-3, GPT-4, Llama-3 etc. have made use fo this tokenizer. \n",
    "\n",
    "OpenAI's original implementation of the BPE tokenizer can be found [here](https://github.com/openai/gpt-2/blob/master/src/encoder.py), while practitioners usually incorporate the [tiktoken](https://github.com/openai/tiktoken) library in their model development pipelines. Karpathy's [minBPE](https://github.com/karpathy/minbpe) is also mentioned in Sebastian's work, as a possible alternative to the worflow below.\n",
    "\n",
    "For practice, the BPE tokenizer will be implemented from scratch in this notebook - though it won't be nearly as optimized as OpenAI's or maybe even Karpathy's versions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046b45e",
   "metadata": {},
   "source": [
    "## **BPE Algorithm Outline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b72c76",
   "metadata": {},
   "source": [
    ">**1. Identify frequent pairs**\n",
    ">- In each iteration, scan the text to find the most commonly occurring pair of bytes (or characters)\n",
    ">\n",
    ">**2. Replace and record**\n",
    ">\n",
    ">- Replace that pair with a new placeholder ID (one not already in use, e.g., if we start with 0...255, the first placeholder would be 256)\n",
    ">- Record this mapping in a lookup table\n",
    ">- The size of the lookup table is a hyperparameter, also called \"vocabulary size\" (for GPT-2, that's\n",
    ">50,257)\n",
    ">\n",
    ">**3. Repeat until no gains**\n",
    ">\n",
    ">- Keep repeating steps 1 and 2, continually merging the most frequent pairs\n",
    ">- Stop when no further compression is possible (e.g., no pair occurs more than once)\n",
    ">\n",
    ">**Decompression (decoding)**\n",
    ">\n",
    ">- To restore the original text, reverse the process by substituting each ID with its corresponding pair, using the lookup table\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b8eab",
   "metadata": {},
   "source": [
    "### **Working Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2fc3cd",
   "metadata": {},
   "source": [
    ">&nbsp;\n",
    "> Suppose we have the text (training dataset) `the cat in the hat` from which we want to build the vocabulary for a BPE tokenizer\n",
    ">\n",
    ">**Iteration 1**\n",
    ">\n",
    ">1. Identify frequent pairs\n",
    ">  - In this text, \"th\" appears twice (at the beginning and before the second \"e\")\n",
    ">\n",
    ">2. Replace and record\n",
    ">  - replace \"th\" with a new token ID that is not already in use, e.g., 256\n",
    ">  - the new text is: `<256>e cat in <256>e hat`\n",
    ">  - the new vocabulary is\n",
    ">\n",
    ">```\n",
    ">  0: ...\n",
    ">  ...\n",
    ">  256: \"th\"\n",
    ">```\n",
    ">\n",
    ">**Iteration 2**\n",
    ">\n",
    ">1. **Identify frequent pairs**  \n",
    ">   - In the text `<256>e cat in <256>e hat`, the pair `<256>e` appears twice\n",
    ">\n",
    ">2. **Replace and record**  \n",
    ">   - replace `<256>e` with a new token ID that is not already in use, for example, `257`.  \n",
    ">   - The new text is:\n",
    ">     ```\n",
    ">     <257> cat in <257> hat\n",
    ">     ```\n",
    ">   - The updated vocabulary is:\n",
    ">     ```\n",
    ">     0: ...\n",
    ">     ...\n",
    ">     256: \"th\"\n",
    ">     257: \"<256>e\"\n",
    ">     ```\n",
    ">\n",
    ">**Iteration 3**\n",
    ">\n",
    ">1. **Identify frequent pairs**  \n",
    ">   - In the text `<257> cat in <257> hat`, the pair `<257> ` appears twice (once at the beginning and once before “hat”).\n",
    ">\n",
    ">2. **Replace and record**  \n",
    ">   - replace `<257> ` with a new token ID that is not already in use, for example, `258`.  \n",
    ">   - the new text is:\n",
    ">     ```\n",
    ">     <258>cat in <258>hat\n",
    ">     ```\n",
    ">   - The updated vocabulary is:\n",
    ">     ```\n",
    ">     0: ...\n",
    ">     ...\n",
    ">     256: \"th\"\n",
    ">     257: \"<256>e\"\n",
    ">     258: \"<257> \"\n",
    ">     ```\n",
    ">     \n",
    ">- and so forth\n",
    ">\n",
    ">&nbsp;\n",
    ">#### Decoding Steps:\n",
    ">\n",
    ">- To restore the original text, we reverse the process by substituting each token ID with its corresponding pair in the reverse order they were introduced\n",
    ">- Start with the final compressed text: `<258>cat in <258>hat`\n",
    ">-  Substitute `<258>` → `<257> `: `<257> cat in <257> hat`  \n",
    ">- Substitute `<257>` → `<256>e`: `<256>e cat in <256>e hat`\n",
    ">- Substitute `<256>` → \"th\": `the cat in the hat`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075aad61",
   "metadata": {},
   "source": [
    "## **Simplified BPE Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f512f4",
   "metadata": {},
   "source": [
    "This is a simplified implementation of the BPE algorithm, which will mimic the `tiktoken` UI. Here the `encode()` method will approximate the original `train()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2cafa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "628c2083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizerLocal:\n",
    "    def __init__(self):\n",
    "        # Map token_id to token_str   \n",
    "        self.vocab = {}\n",
    "        # Map token_str to token_od\n",
    "        self.inverse_vocab = {}\n",
    "        # Dict of BPE merges\n",
    "        self.bpe_merges = {}\n",
    "        # Use a rank dict for GPT-2 merges. Low ranks have higher priority\n",
    "        self.bpe_ranks = {}\n",
    "     \n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        Train BPE tokenizer from scratch\n",
    "\n",
    "        Args:\n",
    "            text (str): Input / training text\n",
    "            vocab_size (int): Desired vocabulary size\n",
    "            allowed_special (set): Set of special tokens to include.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Preprocessing: Replace spaces with \"Ġ\", as implemented in GPT-2.\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "        \n",
    "        # Initialize vocab with unique characters, including \"Ġ\" if present starting\n",
    "        # with first 256 ASCII characters\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "        unique_chars.extend(\n",
    "            char for char in sorted(set(processed_text))\n",
    "            if char not in unique_chars\n",
    "        )\n",
    "        if \"Ġ\" not in unique_chars:\n",
    "            unique_chars.append(\"Ġ\")\n",
    "            \n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "        \n",
    "        # Add allowed special tokens\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "        \n",
    "        # Tokenize the processed_text into token Ids\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "        \n",
    "        # BPE steps: Repeatedly find and replace frequent pairs\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode='most')\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "            \n",
    "        # Build vocab with merged tokens\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "            \n",
    "    def load_vocab_and_merges_from_openai(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load pretained vocabulary and BPE merges from OpenAI's GPT-2 files\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocab file (GPT-2 calls it 'encoder.json)\n",
    "            bpe_merges_path (str): Path to bpe_merges file (GPT-2 calls it 'vocab.bpe'). \n",
    "        \"\"\"\n",
    "        # Load vocab\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            # Load vocab to correct format\n",
    "            self.vocab = {int(v): k for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {k: int(v) for k, v in loaded_vocab.items()}\n",
    "        \n",
    "        # Handle newline character without adding a new token\n",
    "        if \"\\n\" not in self.inverse_vocab:\n",
    "            # Use existing token ID as a placeholder for '\\n' i.e. \"<|endoftext|>\" if available\n",
    "            fallback_token = next((token for token in [\"<|endoftext|>\", \"Ġ\", \"\"] if token in self.inverse_vocab), None)\n",
    "            if fallback_token is not None:\n",
    "                newline_token_id = self.inverse_vocab[fallback_token]\n",
    "            else:\n",
    "                raise KeyError(\"No suitable token found in vocabulary to map '\\\\n'.\")\n",
    "            \n",
    "            self.inverse_vocab[\"\\n\"] = newline_token_id\n",
    "            self.vocab[newline_token_id]= \"\\n\"\n",
    "            \n",
    "        # Load GPT-2 merges and store these with an assigned rank.\n",
    "        self.bpe_ranks = {}\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            if lines and lines[0].startswith(\"#\"):\n",
    "                lines = lines[1:]\n",
    "            \n",
    "            rank = 0\n",
    "            for line in lines:\n",
    "                pair = tuple(line.strip().split())\n",
    "                if len(pair) == 2:\n",
    "                    token1, token2 = pair\n",
    "                    # if both tokens are not in vocab then skip\n",
    "                    if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
    "                        self.bpe_ranks[(token1, token2)] = rank\n",
    "                        rank += 1\n",
    "                    else:\n",
    "                        print(f\"Skipping pair {pair} since one token isn't in the vocabulary!\")\n",
    "  \n",
    "    def encode(self, text, allowed_special=None):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs, with tiktoken style handling of special tokens.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The input text to encode.\n",
    "            allowed_special (set or None): Special tokens to allow passthrough. If None, special handling is disabled.\n",
    "    \n",
    "        Returns:\n",
    "            List of token IDs.\n",
    "        \"\"\"           \n",
    "        import re\n",
    "        token_ids = []\n",
    "        \n",
    "        # If special token handling is enabled\n",
    "        if allowed_special is not None and len(allowed_special) > 0:\n",
    "            # Regex to match allowed special tokens\n",
    "            special_pattern = (\n",
    "                \"(\" + \"|\".join(re.escape(tok) for tok in sorted(allowed_special, key=len, reverse=True)) + \")\"\n",
    "            )\n",
    "            \n",
    "            last_index = 0\n",
    "            for match in re.finditer(special_pattern, text):\n",
    "                prefix = text[last_index:match.start()]\n",
    "                token_ids.extend(self.encode(prefix, allowed_special=None)) # Encode prefix without special handling\n",
    "                \n",
    "                special_token = match.group(0)\n",
    "                if special_token in self.inverse_vocab:\n",
    "                    token_ids.append(self.inverse_vocab[special_token])\n",
    "                else:\n",
    "                    raise ValueError(f\"Special token {special_token} not found in vocabulary!\")\n",
    "                last_index = match.end()\n",
    "            # Normal processing of remaining parts\n",
    "            text = text[last_index:]\n",
    "            \n",
    "            # Check for disallowed special special tokens in the remainder\n",
    "            disallowed = [\n",
    "                tok for tok in self.inverse_vocab\n",
    "                if tok.startswith(\"<|\") and tok.endswith(\"|>\") and tok in text and tok not in allowed_special\n",
    "            ]\n",
    "            if disallowed:\n",
    "                raise ValueError(f\"Disallowed special tokens encountered in text: {disallowed}\")\n",
    "        \n",
    "        # In case of no special tokens , or remaining text after special token split:\n",
    "        tokens = []\n",
    "        lines = text.split(\"\\n\")\n",
    "        for i, line in enumerate(lines):\n",
    "            if i > 0: \n",
    "                tokens.append(\"\\n\")\n",
    "            words = line.split()\n",
    "            for j, word in enumerate(words):\n",
    "                if j == 0 and i > 0:\n",
    "                    tokens.append(\"Ġ\" + word)\n",
    "                elif j == 0:\n",
    "                    tokens.append(word)\n",
    "                else:\n",
    "                    tokens.append(\"Ġ\" + word)\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                token_ids.append(self.inverse_vocab[token])\n",
    "            else:\n",
    "                token_ids.extend(self.tokenize_with_bpe(token))\n",
    "        \n",
    "        return token_ids\n",
    "    \n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \"\"\"\n",
    "        Tokenize a single token using BPE merges.\n",
    "\n",
    "        Args:\n",
    "            token (str): The token to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs after applying BPE.\n",
    "        \"\"\"\n",
    "        # Tokenize token into individal characters \n",
    "        token_ids  = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"Characters not found in vocab: {missing_chars}\")\n",
    "        \n",
    "        # In case OpenAI's GPT-2 merges weren't loaded, run the following\n",
    "        if not self.bpe_ranks:\n",
    "            can_merge = True\n",
    "            while can_merge and len(token_ids) > 1:\n",
    "                can_merge = False\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(token_ids) - 1:\n",
    "                    pair = (token_ids[i], token_ids[i + 1])\n",
    "                    if pair in self.bpe_merges:\n",
    "                        merged_token_id = self.bpe_merges[pair]\n",
    "                        new_tokens.append(merged_token_id)\n",
    "                        # Skip the next token as it is merged\n",
    "                        i += 2 \n",
    "                        can_merge = True\n",
    "                    else:\n",
    "                        new_tokens.append(token_ids[i])\n",
    "                        i += 1\n",
    "                if  i < len(token_ids):\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                token_ids = new_tokens\n",
    "            return token_ids\n",
    "        \n",
    "        # Alternatively run GPT-2 style merging with ranking:\n",
    "        # Convert token_ids back to string \"symbols\" for each ID\n",
    "        symbols = [self.vocab[id_num] for id_num in token_ids]\n",
    "        \n",
    "        # Repeatedly merge all occurences of the lowest-rank pair.\n",
    "        while True:\n",
    "            # Collect all adjacent pairs\n",
    "            pairs = set(zip(symbols, symbols[1:]))\n",
    "            if not pairs: \n",
    "                break\n",
    "            \n",
    "            # Find the pair with the best / lowest rank\n",
    "            min_rank = float(\"inf\")\n",
    "            bigram = None\n",
    "            for p in pairs:\n",
    "                r = self.bpe_ranks.get(p, float(\"inf\"))\n",
    "                if r < min_rank:\n",
    "                    min_rank = r\n",
    "                    bigram = p\n",
    "                \n",
    "            # If no valid ranked pair is present, terminate\n",
    "            if bigram is None or bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            \n",
    "            # Merge all occurence of the pair in question\n",
    "            first, second = bigram\n",
    "            new_symbols = []\n",
    "            i = 0\n",
    "            while i < len(symbols):\n",
    "                # In case of (first, second) at position i, merge\n",
    "                if i < len(symbols) - 1 and symbols[i] == first and symbols[i+1] == second:\n",
    "                    new_symbols.append(first + second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "            symbols = new_symbols\n",
    "            \n",
    "            if len(symbols) == 1:\n",
    "                break\n",
    "        \n",
    "        # Convert merged symbols back to IDs\n",
    "        merged_ids = [self.inverse_vocab[sym] for sym in symbols]\n",
    "        return merged_ids\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into a string.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): The list of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        decoded_string = \"\"\n",
    "        for i, token_id in enumerate(token_ids):\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"Token ID {token_id} not found in vocab.\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token == \"\\n\":\n",
    "                if decoded_string and not decoded_string.endswith(\" \"):\n",
    "                    decoded_string += \" \"  # Add space if not present before a newline\n",
    "                decoded_string += token\n",
    "            elif token.startswith(\"Ġ\"):\n",
    "                decoded_string += \" \" + token[1:]\n",
    "            else:\n",
    "                decoded_string += token\n",
    "        return decoded_string\n",
    "\n",
    "    def save_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Save the vocabulary and BPE merges to JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to save the vocabulary.\n",
    "            bpe_merges_path (str): Path to save the BPE merges.\n",
    "        \"\"\"\n",
    "        # Save vocabulary\n",
    "        with open(vocab_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(self.vocab, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Save BPE merges as a list of dictionaries\n",
    "        with open(bpe_merges_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            merges_list = [{\"pair\": list(pair), \"new_id\": new_id}\n",
    "                           for pair, new_id in self.bpe_merges.items()]\n",
    "            json.dump(merges_list, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def load_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load the vocabulary and BPE merges from JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocabulary file.\n",
    "            bpe_merges_path (str): Path to the BPE merges file.\n",
    "        \"\"\"\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            self.vocab = {int(k): v for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {v: int(k) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # Load BPE merges\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            merges_list = json.load(file)\n",
    "            for merge in merges_list:\n",
    "                pair = tuple(merge[\"pair\"])\n",
    "                new_id = merge[\"new_id\"]\n",
    "                self.bpe_merges[pair] = new_id\n",
    "                \n",
    "    \n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        return self.inverse_vocab.get(token, None)\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if not pairs:\n",
    "            return None\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "        \n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                # Remove 2nd token since 1st was already removed.\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "        \n",
    "        return replaced\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47a1941",
   "metadata": {},
   "source": [
    "## **Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4eb458",
   "metadata": {},
   "source": [
    "### Train, encode and decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cdf63b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c07ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_if_absent(url, filename, search_dirs):\n",
    "    for directory in search_dirs:\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"{filename} already exists in {file_path}\")\n",
    "            return file_path\n",
    "        \n",
    "    target_path = os.path.join(search_dirs[0], filename)\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response, open(target_path, \"wb\") as out_file:\n",
    "            out_file.write(response.read())\n",
    "        print(f\"Downloaded {filename} to {target_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {filename}. Error: {e}\")\n",
    "    return target_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbacaf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the-law.txt already exists in ./the-law.txt\n"
     ]
    }
   ],
   "source": [
    "law_path = download_file_if_absent(\n",
    "    url=(\n",
    "        \"https://github.com/bachaudhry/my-llm-from-scratch/blob/main/data/the-law-bastiat.txt\"\n",
    "    ),\n",
    "    filename=\"the-law.txt\",\n",
    "    search_dirs=\".\"\n",
    ")\n",
    "\n",
    "with open(law_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13665215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BPE Tokenizer\n",
    "tokenizer = BPETokenizerLocal()\n",
    "tokenizer.train(text, vocab_size=1000, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18c626c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab:  1000 \n",
      "BPE Merges:  739\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab: \", len(tokenizer.vocab), \"\\nBPE Merges: \", (len(tokenizer.bpe_merges)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1115016",
   "metadata": {},
   "source": [
    "- The vocabulary size is already 256 by default, based on the single character ASCII tokens we've factored into the tokenizer. This way the tokenizer learns 739 vocabulary entries (including the `<|endoftext|>` and `Ġ` special tokens). \n",
    "- The GPT-2 tokenizer vocabulary is 50,257 tokens while GPT-4o takes it to 199,997 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c775399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79, 110, 101, 256, 307, 470, 465, 509, 461, 256, 987, 500, 256, 307, 470, 351, 392, 110, 388, 256, 119, 522, 256, 301, 256, 273, 302, 413, 481, 101, 256, 481, 392, 99, 386, 741, 101, 46]\n",
      "\n",
      " 38\n"
     ]
    }
   ],
   "source": [
    "input_text = \"One of the first cares of the prince was to encourage agriculture.\"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)\n",
    "print(\"\\n\", len(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74c9628b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79, 110, 101, 256, 307, 470, 465, 509, 461, 256, 987, 500, 256, 307, 470, 351, 392, 110, 388, 256, 119, 522, 256, 301, 256, 273, 302, 413, 481, 101, 256, 481, 392, 99, 386, 741, 101, 46, 60, 124, 740, 307, 116, 562, 124, 62]\n",
      "\n",
      " 46\n"
     ]
    }
   ],
   "source": [
    "input_text = \"One of the first cares of the prince was to encourage agriculture.<|endoftext|>\"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)\n",
    "print(\"\\n\", len(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "251fde90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79, 110, 101, 256, 307, 470, 465, 509, 461, 256, 987, 500, 256, 307, 470, 351, 392, 110, 388, 256, 119, 522, 256, 301, 256, 273, 302, 413, 481, 101, 256, 481, 392, 99, 386, 741, 101, 46, 260]\n",
      "\n",
      " 39\n"
     ]
    }
   ],
   "source": [
    "input_text = \"One of the first cares of the prince was to encourage agriculture.<|endoftext|>\"\n",
    "token_ids = tokenizer.encode(input_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_ids)\n",
    "print(\"\\n\", len(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78785f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 79\n",
      "Number of token IDs: 39\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(input_text))\n",
    "print(\"Number of token IDs:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4f0667",
   "metadata": {},
   "source": [
    "- Here, the 79 character sentence was encoded into 39 token IDs, which illustrates the compression component of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "990f4e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the first cares of the prince was to encourage agriculture.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fcd8590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 ---> O\n",
      "110 ---> n\n",
      "101 ---> e\n",
      "256 --->  \n",
      "307 ---> of\n",
      "470 --->  the\n",
      "465 --->  f\n",
      "509 ---> ir\n",
      "461 ---> st\n",
      "256 --->  \n",
      "987 ---> ca\n",
      "500 ---> res\n",
      "256 --->  \n",
      "307 ---> of\n",
      "470 --->  the\n",
      "351 --->  p\n",
      "392 ---> ri\n",
      "110 ---> n\n",
      "388 ---> ce\n",
      "256 --->  \n",
      "119 ---> w\n",
      "522 ---> as\n",
      "256 --->  \n",
      "301 ---> to\n",
      "256 --->  \n",
      "273 ---> en\n",
      "302 ---> co\n",
      "413 ---> ur\n",
      "481 ---> ag\n",
      "101 ---> e\n",
      "256 --->  \n",
      "481 ---> ag\n",
      "392 ---> ri\n",
      "99 ---> c\n",
      "386 ---> ul\n",
      "741 ---> tur\n",
      "101 ---> e\n",
      "46 ---> .\n",
      "260 ---> <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Iterating over each token ID\n",
    "for id in token_ids:\n",
    "    print(f\"{id} ---> {tokenizer.decode([id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedd3d1f",
   "metadata": {},
   "source": [
    "### Saving and Loading the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5f52eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer\n",
    "tokenizer.save_vocab_and_merges(vocab_path=\"output/vocab.json\", bpe_merges_path=\"output/bpe_merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d59393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer2 = BPETokenizerLocal()\n",
    "tokenizer2.load_vocab_and_merges(vocab_path=\"output/vocab.json\", bpe_merges_path=\"output/bpe_merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dd6ed1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the first cares of the prince was to encourage agriculture.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer2.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39c5d89",
   "metadata": {},
   "source": [
    "### Loading the GPT-2 BPE from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d781dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab.bpe already exists in ../supplementary/output/gpt2-model/vocab.bpe\n",
      "encoder.json already exists in ../supplementary/output/gpt2-model/encoder.json\n"
     ]
    }
   ],
   "source": [
    "# Download necessary files\n",
    "search_dir = [\".\", \"../supplementary/output/gpt2-model/\"]\n",
    "\n",
    "files_to_download = {\n",
    "    \"https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe\": \"vocab.bpe\",\n",
    "    \"https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json\": \"encoder.json\"\n",
    "}\n",
    "\n",
    "# Ensure directories exist and download \n",
    "paths = {}\n",
    "for url, filename in files_to_download.items():\n",
    "    paths[filename] = download_file_if_absent(url, filename, search_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a626a957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading files\n",
    "tokenizer_gpt2 = BPETokenizerLocal()\n",
    "tokenizer_gpt2.load_vocab_and_merges_from_openai(\n",
    "    vocab_path=paths[\"encoder.json\"], bpe_merges_path=paths[\"vocab.bpe\"]\n",
    ")\n",
    "\n",
    "len(tokenizer_gpt2.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac6330f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 257, 2420, 6291, 13]\n"
     ]
    }
   ],
   "source": [
    "# Testing the GPT tokenizer\n",
    "input_text = \"This is a text sample.\"\n",
    "token_ids = tokenizer_gpt2.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddc3f963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text sample.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_gpt2.decode(token_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
