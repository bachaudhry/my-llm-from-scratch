# Results of Local Experiments - Classification of Sentiment of 50k IMDb Movie Reviews 


## Overview

This folder contains experiments focused on the comparison of a decoder-style GPT-2 model and encoder-style LLMS like [BERT (2018)](https://arxiv.org/abs/1810.04805), [RoBERTa (2019)](https://arxiv.org/abs/1907.11692), and [ModernBERT (2024)](https://arxiv.org/abs/2412.13663).

The work is based on my localized versions of [Sebastian Raschka's experiments](https://github.com/rasbt/LLMs-from-scratch/tree/main/ch06/03_bonus_imdb-classification), which utilze the [50k IMDb movie review dataset](https://ai.stanford.edu/~amaas/data/sentiment/) with a binary classification objective predicting whether a reviewer liked or disliked a movie.


## Experiment Results

|       | Model                        | Test accuracy |
| ----- | ---------------------------- | ------------- |
| **1** | Logistic Regression Baseline | 88.77%        |
| **2** | 124M GPT-2 Baseline          | ------        |
| **3** | 340M BERT                    | 91.43%        |
| **4** | 66M DistilBERT               | 90.53%        |
| **5** | 355M RoBERTa                 | 92.15%        |
| **6** | 304M DeBERTa-v3              | 94.33%        |
| **7** | 149M ModernBERT Base         | 93.77%        |
| **8** | 395M ModernBERT Large        | ------        |


## Model Architecture Modifications _(generated by Gemini)_


#### 1. `distilbert-base-uncased`

- **Core Idea:** A distilled, smaller, and faster version of BERT that retains most of its performance.

- **Architectural Changes:**

    - `model = AutoModelForSequenceClassification.from_pretrained(...)`: This loads the DistilBERT architecture with a pre-defined sequence classification head.

    - `model.out_head = torch.nn.Linear(in_features=768, out_features=2)`: The final classification layer in DistilBERT's SequenceClassification model is called out_head. The script explicitly replaces it with a new linear layer. The in_features=768 corresponds to the hidden dimension size of the distilbert-base model.

    - Why this name? The designers of the DistilBERT model in the transformers library named the final layer out_head.

- **`last_block` Logic:**

    - `model.distilbert.transformer.layer[-1]`: This unfreezes the final Transformer encoder block. This is a common fine-tuning strategy to allow the highest-level feature representations to adapt.

    - `model.pre_classifier`: DistilBERT has an additional pre_classifier layer (a Linear layer with a ReLU activation) that sits between the main Transformer blocks and the final out_head. To fine-tune the "block," it's logical to include this layer as well.

#### 2. `bert-base-uncased`

 - **Core Idea:** The original, foundational Transformer encoder model.

- **Architectural Changes:**

    - `model.classifier = torch.nn.Linear(in_features=768, out_features=2)`: In the original BERT implementation for sequence classification, the final layer is simply named classifier. We replace it for our binary task.

- **`last_block` Logic:**

    - `model.bert.encoder.layer[-1]`: Unfreezes the final Transformer encoder block, similar to DistilBERT.

    - `model.bert.pooler.dense`: This is a key component of BERT. The Pooler is responsible for taking the output of the final Transformer block (specifically the hidden state corresponding to the special [CLS] token) and transforming it via a dense layer and a Tanh activation. This pooled output is what's fed to the classifier. To properly fine-tune the final "block," you must include the Pooler.

#### 3. `FacebookAI/roberta-large`

- **Core Idea**: A "Robustly Optimized" version of BERT with improved pre-training techniques.

- **Architectural Changes:**

    - `model.classifier.out_proj = torch.nn.Linear(in_features=1024, out_features=2)`: RoBERTa's classification head is slightly more complex. It's a module called classifier which contains an internal dense layer and then a final output projection layer called out_proj. We are replacing only this final projection. Note the `in_features=1024`, which is the hidden dimension for a large model variant.

- **`last_block` Logic:**

    - `model.roberta.encoder.layer[-1]`: Unfreezes the final encoder block, located within the roberta main module.

    - `for param in model.classifier.parameters()`: Instead of just targeting the out_proj, the script unfreezes the entire classifier module, which is a reasonable choice for fine-tuning the last "block."

#### 4. answerdotai/ModernBERT

- **Core Idea:** A more recent variant with architectural tweaks. The key takeaway is its unique internal structure.

- **Architectural Changes:**

    - `model.classifier = torch.nn.Linear(...)`: Like BERT, the final layer is conveniently named classifier.

- **`last_block` Logic:**

    - `model.model.layers[-1]`: The path to the final Transformer block is different here, nested inside model.model. This highlights the necessity of model-specific code.

    - `model.head`: This model appears to have a distinct "head" module separate from the final classifier, which is also unfrozen.

#### 5. microsoft/deberta-v3-base

- **Core Idea:** An advanced architecture featuring a "disentangled attention" mechanism.

- **Architectural Changes:**

    - `model.classifier = torch.nn.Linear(in_features=768, out_features=2)`: The final classification layer is again named classifier.

- **last_block Logic:**

    - `model.deberta.encoder.layer[-1]`: Unfreezes the final encoder block within the deberta main module.

    - `model.pooler`: Similar to BERT, DeBERTa has a pooler module that processes the encoder output before it goes to the classifier. It's a top-level module in this architecture (unlike `bert.pooler`), and it is crucial to unfreeze it when fine-tuning the last block.

