{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5433f8",
   "metadata": {},
   "source": [
    "# Comparisons of Different BPE Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e3558",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e247f60",
   "metadata": {},
   "source": [
    "## BPE From tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb62b226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d3ae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11633, 262, 2068, 7586, 21831, 4391, 625, 262, 16931, 6844, 30]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tik_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Did the quick brown fox jump over the lazy dogs?\"\n",
    "\n",
    "integers = tik_tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac4db3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did the quick brown fox jump over the lazy dogs?\n"
     ]
    }
   ],
   "source": [
    "strings = tik_tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e9b511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257\n"
     ]
    }
   ],
   "source": [
    "print(tik_tokenizer.n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695fe27e",
   "metadata": {},
   "source": [
    "## BPE Implementation of GPT-2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e775a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai_gpt2_bpe import get_encoder, download_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81b2ac09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching encoder.json: 1.04Mit [00:11, 92.7kit/s]                                                   \n",
      "Fetching vocab.bpe: 457kit [00:05, 83.3kit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "download_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4803d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "oai_tokenizer = get_encoder(model_name=\"gpt2_model\", models_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee36e1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11633, 262, 2068, 7586, 21831, 4391, 625, 262, 16931, 6844, 30]\n"
     ]
    }
   ],
   "source": [
    "integers = oai_tokenizer.encode(text)\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e3079b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did the quick brown fox jump over the lazy dogs?\n"
     ]
    }
   ],
   "source": [
    "strings = oai_tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd64b17",
   "metadata": {},
   "source": [
    "## BPE via HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3954aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bac/code/nbs/LLMs-from-scratch/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4.51.3'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02004f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "hf_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62f55e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11633, 262, 2068, 7586, 21831, 4391, 625, 262, 16931, 6844, 30]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer(strings)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65df0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "hf_tokenizer_fast = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "093933ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11633, 262, 2068, 7586, 21831, 4391, 625, 262, 16931, 6844, 30]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer_fast(strings)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4921a5",
   "metadata": {},
   "source": [
    "## Sebastian's Local BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f18f3a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, io, nbformat, types\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5772c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_from_notebook():\n",
    "    def import_definitions_from_notebook(fullname, names):\n",
    "        current_dir = os.getcwd()\n",
    "        path = os.path.join(current_dir, \"..\", \"supplementary\", fullname + \".ipynb\")\n",
    "        path = os.path.normpath(path)\n",
    "        \n",
    "        # Load the NB\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Notebook not found at --> {path}\")\n",
    "        \n",
    "        with io.open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "            \n",
    "        # Create module to store the imported funcs and classes\n",
    "        mod = types.ModuleType(fullname)\n",
    "        sys.modules[fullname] = mod\n",
    "        \n",
    "        # Add the notebook's cells to the module's namespac\n",
    "        exec(\"from collections import Counter, deque\", mod.__dict__)\n",
    "        exec(\"from functools import lru_cache\", mod.__dict__)\n",
    "        exec(\"import json\", mod.__dict__)\n",
    "        \n",
    "        # Go through the nb cells and execute func or class definitions\n",
    "        for cell in nb.cells:\n",
    "            if cell.cell_type == \"code\":\n",
    "                cell_code = cell.source\n",
    "                for name in names:\n",
    "                    # Funcs or class definition check\n",
    "                    if f\"def {name}\" in cell_code or f\"class {name}\" in cell_code:\n",
    "                        exec(cell_code, mod.__dict__)\n",
    "        return mod\n",
    "    \n",
    "    fullname = \"01b-bpe-from-scratch\"\n",
    "    names = [\"BPETokenizerLocal\"]\n",
    "    \n",
    "    return import_definitions_from_notebook(fullname, names)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "284563c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_module = import_from_notebook()\n",
    "BPETokenizerLocal = getattr(import_module, \"BPETokenizerLocal\", None)\n",
    "\n",
    "tokenizer_gpt2 = BPETokenizerLocal()\n",
    "tokenizer_gpt2.load_vocab_and_merges_from_openai(\n",
    "    vocab_path=os.path.join(\"gpt2_model\", \"encoder.json\"),\n",
    "    bpe_merges_path=os.path.join(\"gpt2_model\", \"vocab.bpe\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e233be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11633, 262, 2068, 7586, 21831, 4391, 625, 262, 16931, 6844, 30]\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer_gpt2.encode(text)\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e7d7c6",
   "metadata": {},
   "source": [
    "## Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37df44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/the-law-bastiat.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc6ef38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.9 ms ± 56.5 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Using the original OpenAI GPT2-Tokenizer\n",
    "%timeit oai_tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abb0d639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.32 ms ± 30.9 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Using the tiktoken Tokenizer\n",
    "%timeit tik_tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04d00e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.2 ms ± 148 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Using the HuggingFace GPT-2 tokenizer\n",
    "%timeit hf_tokenizer(text)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cf06daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.7 ms ± 420 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace Tokenizer with max length and truncation\n",
    "%timeit hf_tokenizer(text, max_length=5145, truncation=True)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "619fc3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (21939 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.8 ms ± 225 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace Tokenizer - Fast version\n",
    "%timeit hf_tokenizer_fast(text)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da35759c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.5 ms ± 116 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace - Fast Tokenizer with max length and truncation\n",
    "%timeit hf_tokenizer_fast(text, max_length=5145, truncation=True)[\"input_ids\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
