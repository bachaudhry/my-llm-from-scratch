{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "664fc013",
   "metadata": {},
   "source": [
    "# FLOPS Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be12d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"thop\",\n",
    "    \"torch\"\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad0d816",
   "metadata": {},
   "source": [
    "## Simple Benchmark With Fixed Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a6dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from thop import profile\n",
    "from llms_from_scratch.ch04 import GPTModel\n",
    "\n",
    "\n",
    "BASE_CONFIG= {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 2\n",
    "input_tensor = torch.randint(0, 50257, (batch_size, 1024)).to(device)\n",
    "\n",
    "for size in model_configs:\n",
    "    BASE_CONFIG.update(model_configs[size])\n",
    "    \n",
    "    model = GPTModel(BASE_CONFIG).bfloat16()  \n",
    "    model.to(device)\n",
    "    \n",
    "    # MACS --> multiply-accumulate operations\n",
    "    # MACS are counted as 2 FLOPS, one for multiplication and the other for accumulate\n",
    "    macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "    flops = 2 * macs\n",
    "    print(f\"{size:18}: {flops:.1e} FLOPS\")\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24163d29",
   "metadata": {},
   "source": [
    "## Simple Benchmark With Automatic Batch Size Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f774f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This applies to the forward-pass only\n",
    "for size in model_configs:\n",
    "    print(f\"\\nProcessing {size}\")\n",
    "    config = BASE_CONFIG.copy()\n",
    "    config.update(model_configs[size])\n",
    "    \n",
    "    min_batch_size = 1\n",
    "    max_batch_size = None\n",
    "    max_possible_batch_size = 4096\n",
    "    \n",
    "    while min_batch_size <= max_possible_batch_size:\n",
    "        batch_size = (min_batch_size + max_possible_batch_size) // 2\n",
    "        try:\n",
    "            input_tensor = torch.randint(\n",
    "                0, config[\"vocab_size\"],\n",
    "                (batch_size, config[\"context_length\"]),\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            model = GPTModel(config).bfloat16().to(device)\n",
    "            \n",
    "            # MACS = multiply-accumulate operations\n",
    "            # MACS are typically counted as two FLOPS (one multiply and one accumulate)\n",
    "            macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "            flops = 2 * macs\n",
    "            print(f\"  Batch size {batch_size}: {flops:.1e} FLOPS\")\n",
    "            \n",
    "            # If successful, increase the batch size\n",
    "            min_batch_size = batch_size + 1\n",
    "            max_batch_size = batch_size\n",
    "            \n",
    "            del model, input_tensor\n",
    "            torch.cuda.empty_cache()\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                # Try a smaller batch size\n",
    "                max_possible_batch_size - 1\n",
    "                \n",
    "                try:\n",
    "                    del model, input_tensor\n",
    "                    torch.cuda.empty_cache()\n",
    "                except NameError:\n",
    "                    pass\n",
    "            else:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc001709",
   "metadata": {},
   "source": [
    "## Benchmark With Automatic Batch Size Finding and Model FLOP Utilization (MFU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711f9c1",
   "metadata": {},
   "source": [
    "- Model FLOPs Utilization (MFU) explanation from the [PaLM paper](https://arxiv.org/abs/2204.02311)\n",
    "\n",
    "> We propose a new metric for efficiency that is implementation-independent and permits a cleaner comparison of system efficiency, called model FLOPs utilization (MFU). This is the ratio of the observed throughput (tokens-per-second) relative to the theoretical maximum throughput of a system operating at peak FLOPs. Crucially, the “theoretical maximum” throughput only accounts for the required operations to compute the forward+backward passes, and not rematerialization.\n",
    "\n",
    "\n",
    "$$\\text{MFU} = \\frac{\\text{Observed Tokens per Second}}{\\text{Theoretical Max Tokens per Second}}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\text{Theoretical Max Tokens per Second} = \\frac{\\text{Max FLOPs per Second}}{\\text{Total FLOPs per Token}}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\text{Tokens per Second} = \\frac{\\text{Batch Size} \\times \\text{Sequence Length}}{\\text{Total Time}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffdb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical max flops per second provided by the GPU manufacturer\n",
    "\n",
    "flops_per_second = {\n",
    "    # https://www.techpowerup.com/gpu-specs/h100-pcie-80-gb.c3899\n",
    "    \"H100\": {\n",
    "        torch.float32: 51.22e12,  # 51.22 TFLOPs for FP32 on NVIDIA H100\n",
    "        torch.float16: 204.9e12,  # 204.9 TFLOPs for FP16 on NVIDIA H100\n",
    "        torch.bfloat16: 204.9e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/l4.c4091\n",
    "    \"L4\": {\n",
    "        torch.float32: 30.29e12,  # 30.29 TFLOPs for FP32 on NVIDIA L4\n",
    "        torch.float16: 30.29e12,  # 30.29 TFLOPs for FP16 on NVIDIA L4\n",
    "        torch.bfloat16: 30.29e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/tesla-t4.c3316\n",
    "    \"T4\": {\n",
    "        torch.float32: 8.1e12,  # 8.1 TFLOPs for FP32 on NVIDIA T4\n",
    "        torch.float16: 65.13e12,  # 65.13 TFLOPs for FP16 on NVIDIA T4\n",
    "        torch.bfloat16: 65.13e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/a10g.c3798\n",
    "    \"A10G\": {\n",
    "        torch.float32: 31.52e12,  # 31.52 TFLOPs for FP32 on NVIDIA A10G\n",
    "        torch.float16: 31.52e12,  # 31.52 TFLOPs for FP16 on NVIDIA A10G\n",
    "        torch.bfloat16: 31.52e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/a100-pcie-40-gb.c3623\n",
    "    \"A100\": {\n",
    "        torch.float32: 19.49e12,  # 19.49 TFLOPs for FP32 on NVIDIA A100\n",
    "        torch.float16: 77.97e12,  # 77.97 TFLOPs for FP16 on NVIDIA A100\n",
    "        torch.bfloat16: 77.97e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/geforce-rtx-3080.c3621\n",
    "    \"RTX_3080\": {\n",
    "        torch.float32: 29.77e12,  # 29.77 TFLOPs for FP32 on NVIDIA RTX 3080\n",
    "        torch.float16: 29.77e12,  # 29.77 TFLOPs for FP16 on NVIDIA RTX 3080\n",
    "        torch.bfloat16: 29.77e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/geforce-rtx-3090.c3622\n",
    "    \"RTX_3090\": {\n",
    "        torch.float32: 35.58e12,  # 35.58 TFLOPs for FP32 on NVIDIA RTX 3090\n",
    "        torch.float16: 35.58e12,  # 35.58 TFLOPs for FP16 on NVIDIA RTX 3090\n",
    "        torch.bfloat16: 35.58e12\n",
    "    },\n",
    "    # htpps://https://www.techpowerup.com/gpu-specs/geforce-rtx-4080.c3888\n",
    "    \"RTX 4080\": {\n",
    "        torch.float32: 48.74e12,  # 48.74 TFLOPS for FP32 on NVIDIA RTX 4080\n",
    "        torch.float16: 48.74e12,  # 48.74 TFLOPS for FP16 on NVIDIA RTX 4080\n",
    "        torch.bfloat16: 48.74e12\n",
    "    },\n",
    "    # htpps://https://www.techpowerup.com/gpu-specs/geforce-rtx-4080.c3888\n",
    "    \"RTX 4090\": {\n",
    "        torch.float32: 82.58e12,  # 82.58 TFLOPS for FP32 on NVIDIA RTX 4090\n",
    "        torch.float16: 82.58e12,  # 82.58 TFLOPS for FP16 on NVIDIA RTX 4090\n",
    "        torch.bfloat16: 82.58e12\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ee0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_gpu_model(FLOPS_dict):\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    for model in FLOPS_dict.keys():\n",
    "        if model in device_name:\n",
    "            return model\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Test\n",
    "get_gpu_model(flops_per_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_model = get_gpu_model(flops_per_second)\n",
    "print(\"GPU Model: \", gpu_model)\n",
    "\n",
    "if gpu_model != \"Unknown\":\n",
    "    \n",
    "    for size in model_configs:\n",
    "        print(f\"Processing {size}\")\n",
    "        config = BASE_CONFIG.copy()\n",
    "        config.update(model_configs[size])\n",
    "        \n",
    "        min_batch_size = 1\n",
    "        max_batch_size = None\n",
    "        max_possible_batch_size = 4096\n",
    "        \n",
    "        while min_batch_size <= max_possible_batch_size:\n",
    "            batch_size = (min_batch_size + max_possible_batch_size) // 2\n",
    "            try:\n",
    "                input_tensor = torch.randint(\n",
    "                    0, config[\"vocab_size\"],\n",
    "                    (batch_size, config[\"context_length\"]),\n",
    "                    device=device\n",
    "                )\n",
    "                \n",
    "                model = GPTModel(config).bfloat16().to(device)\n",
    "                model.train()\n",
    "                \n",
    "                # Start timing\n",
    "                torch.cuda.synchronize()\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Forward and backward pass\n",
    "                output = model(input_tensor)\n",
    "                loss = output.sum() # Dummy loss\n",
    "                loss.backward()\n",
    "                \n",
    "                # End timing\n",
    "                torch.cuda.synchronize()\n",
    "                end_time = time.time()\n",
    "                \n",
    "                total_time_seconds = end_time - start_time\n",
    "                \n",
    "                # Calcuate FLOPS for forward pass\n",
    "                macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "                flops_forward = 2 * macs # given one MAC equals two FLOPS\n",
    "                \n",
    "                # Estimate FLOPS for backward pass\n",
    "                flops_backward = 2 * flops_forward # given these are typically 2x forward FLOPS\n",
    "                \n",
    "                # Total FLOPS \n",
    "                total_flops = flops_forward + flops_backward\n",
    "                \n",
    "                data_type = next(model.parameters()).dtype\n",
    "                max_flops_per_second = flops_per_second[gpu_model].get(data_type, 0)\n",
    "                \n",
    "                # Compute tokens per second\n",
    "                tokens_processed = batch_size * config[\"context_length\"]\n",
    "                tokens_per_second = tokens_processed / total_time_seconds\n",
    "                \n",
    "                # Compute FLOPS per token\n",
    "                flops_per_token = total_flops / tokens_per_second\n",
    "                \n",
    "                # Compute theoretical max tokens per second\n",
    "                if flops_per_token > 0:\n",
    "                    theoretical_max_tokens_per_second = max_flops_per_second / flops_per_token\n",
    "                else:\n",
    "                    theoretical_max_tokens_per_second = 0 # avoid div by 0\n",
    "                    \n",
    "                # Compute MFU\n",
    "                if theoretical_max_tokens_per_second > 0:\n",
    "                    mfu = tokens_per_second / theoretical_max_tokens_per_second\n",
    "                else:\n",
    "                    mfu = 0\n",
    "                    \n",
    "                print(f\"    Batch size {batch_size}: Tokens/sec: {tokens_per_second:.2f}, MFU: {mfu:.4f}\")\n",
    "                \n",
    "                # If successful, try a larger batch size\n",
    "                min_batch_size = batch_size + 1\n",
    "                max_batch_size = batch_size\n",
    "                \n",
    "                # Clean up\n",
    "                del model, input_tensor, output, loss\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    # Try smaller batch size\n",
    "                    max_possible_batch_size = batch_size - 1\n",
    "\n",
    "                    # Clean up\n",
    "                    try:\n",
    "                        del model, input_tensor\n",
    "                        torch.cuda.empty_cache()\n",
    "                    except NameError:\n",
    "                        pass\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "else:\n",
    "    print(\"Unknown GPU model. Refer to the manufacturer's product page for information on you GPU.\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f48e99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
